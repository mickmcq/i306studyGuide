{
  "hash": "6746cee7795757dfffaf650f0fce7dae",
  "result": {
    "markdown": "---\ntitle: \"Details of `ggplot2`; Introduction to Inference\"\n---\n\n\n\n\n## Recap Week 05\n\n- Normal distribution\n- Geometric distribution\n- Binomial distribution\n- Negative binomial distribution\n- Poisson distribution\n\n## ggplot2\nWe'll follow the `ggplot2` cheatsheet, available at\n[https://posit.co/wp-content/uploads/2022/10/data-visualization-1.pdf](https://posit.co/wp-content/uploads/2022/10/data-visualization-1.pdf).\n\nBy the way, there are many more cheatsheets for different aspects of R and RStudio available at\n[https://posit.co/resources/cheatsheets/](https://posit.co/resources/cheatsheets/).\n\n## Base layer\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = mpg, aes(x = cty, y = hwy))\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nThe above code doesn't produce a plot by itself. We would have to add a layer, such as a geom or stat. (Every geom has a default stat and every stat has a default geom.) For example,\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = mpg, aes(x = cty, y = hwy)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nadds a scatterplot, whereas\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = mpg, aes(x = cty, y = hwy)) +\n  geom_point(aes(color=displ,size=displ))\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nadds the aesthetics from the second example on the cheatsheet.\n\n### Graphical primitives\n\n\n::: {.cell}\n\n```{.r .cell-code}\na <- ggplot(economics,aes(date,unemploy))\nb <- ggplot(seals,aes(x=long,y=lat))\na + geom_blank()\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\na + expand_limits()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in max(lengths(data)): no non-missing arguments to max; returning -Inf\n```\n:::\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-5-2.png){width=672}\n:::\n\n```{.r .cell-code}\nb + geom_curve(aes(yend = lat + 1, xend = long + 1), curvature = 1)\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-5-3.png){width=672}\n:::\n\n```{.r .cell-code}\na + geom_path(lineend = \"butt\", linejoin = \"round\", linemitre = 1)\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-5-4.png){width=672}\n:::\n\n```{.r .cell-code}\na + geom_polygon(aes(alpha = 50))\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-5-5.png){width=672}\n:::\n\n```{.r .cell-code}\nb + geom_rect(aes(xmin = long, ymin = lat, xmax = long + 1, ymax = lat + 1))\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-5-6.png){width=672}\n:::\n\n```{.r .cell-code}\na + geom_ribbon(aes(ymin = unemploy - 900, ymax = unemploy + 900))\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-5-7.png){width=672}\n:::\n:::\n\n\n#### Line segments\n\n\n::: {.cell}\n\n```{.r .cell-code}\nb + geom_abline(aes(intercept = 0, slope = 1))\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\nb + geom_hline(aes(yintercept = lat))\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n\n```{.r .cell-code}\nb + geom_vline(aes(xintercept = long))\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-6-3.png){width=672}\n:::\n:::\n\n\n### One variable---continuous\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc <- ggplot(mpg, aes(hwy)); c2 <- ggplot(mpg)\nc + geom_area(stat = \"bin\")\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\nc + geom_density(kernel = \"gaussian\")\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-7-2.png){width=672}\n:::\n\n```{.r .cell-code}\nc + geom_dotplot()\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-7-3.png){width=672}\n:::\n\n```{.r .cell-code}\nc + geom_freqpoly()\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-7-4.png){width=672}\n:::\n\n```{.r .cell-code}\nc + geom_histogram(binwidth = 5)\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-7-5.png){width=672}\n:::\n\n```{.r .cell-code}\nc2 + geom_qq(aes(sample = hwy))\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-7-6.png){width=672}\n:::\n:::\n\n\n### One variable---discrete\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- ggplot(mpg, aes(fl))\nd + geom_bar()\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n### Two variables---both continuous\n\n\n::: {.cell}\n\n```{.r .cell-code}\ne <- ggplot(mpg,aes(cty,hwy))\ne + geom_label(aes(label = cty), nudge_x = 1, nudge_y = 1)\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\ne + geom_point()\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-9-2.png){width=672}\n:::\n\n```{.r .cell-code}\ne + geom_quantile()\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-9-3.png){width=672}\n:::\n\n```{.r .cell-code}\ne + geom_rug(sides = \"bl\")\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-9-4.png){width=672}\n:::\n\n```{.r .cell-code}\ne + geom_smooth(method = lm)\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-9-5.png){width=672}\n:::\n\n```{.r .cell-code}\ne + geom_text(aes(label = cty), nudge_x = 1, nudge_y = 1)\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-9-6.png){width=672}\n:::\n:::\n\n\n### Two variables---one discrete, one continuous\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf <- ggplot(mpg,aes(class,hwy))\nf + geom_col()\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n```{.r .cell-code}\nf + geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-10-2.png){width=672}\n:::\n\n```{.r .cell-code}\nf + geom_dotplot(binaxis = \"y\", stackdir = \"center\")\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-10-3.png){width=672}\n:::\n\n```{.r .cell-code}\nf + geom_violin(scale = \"area\")\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-10-4.png){width=672}\n:::\n:::\n\n\n### Two variables---both discrete\n\n\n::: {.cell}\n\n```{.r .cell-code}\ng <- ggplot(diamonds, aes(cut, color))\ng + geom_count()\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n```{.r .cell-code}\ng + geom_jitter(height = 2, width = 2)\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-11-2.png){width=672}\n:::\n:::\n\n\n### Continuous bivariate distribution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nh <- ggplot(diamonds, aes(carat, price))\nh + geom_bin2d(binwidth = c(0.25, 500))\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n\n```{.r .cell-code}\nh + geom_density_2d()\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-12-2.png){width=672}\n:::\n\n```{.r .cell-code}\nh + geom_hex()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Computation failed in `stat_binhex()`\nCaused by error in `compute_group()`:\n! The package \"hexbin\" is required for `stat_binhex()`\n```\n:::\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-12-3.png){width=672}\n:::\n:::\n\n\n### Continuous function\n\n\n::: {.cell}\n\n```{.r .cell-code}\ni <- ggplot(economics, aes(date, unemploy))\ni + geom_area()\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n\n```{.r .cell-code}\ni + geom_line()\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-13-2.png){width=672}\n:::\n\n```{.r .cell-code}\ni + geom_step(direction = \"hv\")\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-13-3.png){width=672}\n:::\n:::\n\n\n### Visualizing error\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- data.frame(grp = c(\"A\", \"B\"), fit = 4:5, se = 1:2)\nj <- ggplot(df, aes(grp, fit, ymin = fit - se, ymax = fit + se))\nj + geom_crossbar(fatten = 2)\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n```{.r .cell-code}\nj + geom_errorbar()\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-14-2.png){width=672}\n:::\n\n```{.r .cell-code}\nj + geom_linerange()\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-14-3.png){width=672}\n:::\n\n```{.r .cell-code}\nj + geom_pointrange()\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-14-4.png){width=672}\n:::\n:::\n\n\n### Maps\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- data.frame(murder = USArrests$Murder,\nstate = tolower(rownames(USArrests)))\nmap <- map_data(\"state\")\nk <- ggplot(data, aes(fill = murder))\nk + geom_map(aes(map_id = state), map = map) + expand_limits(x = map$long, y = map$lat)\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nThere is more, but you can go through that on your own and, if you wish, add it to this file.\n\n## Additional plots\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ndf <- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/vehicles.csv\"))\ndf$cylinders <- as_factor(df$cylinders)\ndf <- df[df$price<500000&df$price>500,]\ndf<-df[df$cylinders==\"10 cylinders\"|df$cylinders==\"12 cylinders\",]\ndf$price<-as.numeric(df$price)\noptions(scipen=999)\ndf |> ggplot(aes(price,cylinders))+geom_boxplot()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 156391 rows containing non-finite values (`stat_boxplot()`).\n```\n:::\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndfa <- df |> count(condition,cylinders)\nlibrary(treemap)\npalette.HCL.options <- list(hue_start=270, hue_end=360+150)\ntreemap(dfa,\n  index=c(\"condition\",\"cylinders\"),\n  vSize=\"n\",\n  type=\"index\",\n  palette=\"Reds\"\n)\n```\n\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nThe palette in this case is selected from RColorbrewer. You can find the full set of RColorbrewer palettes at the R Graph Gallery.\n\n## Foundations for Inference\n\nThe following material comes from Mick McQuaid's study guide for a previous course. In that course, we used @Mendenhall2012 as a textbook, so there are innumerable references to that textbook in the following material. You don't actually need that textbook and I will eventually delete references to it from this file. I just don't have time right now.\n\n## normal distribution\n\nThis picture illustrates the normal distribution. The\nmound-shaped curve represents the probability density\nfunction and the area between the curve and the\nhorizontal line represents the value of the cumulative distribution\nfunction.\n\n\n::: {.cell engine.opts='{\"extra.preamble\":[\"\\\\usepackage{pgfplots,stix,pagecolor}\",\"\\\\pgfplotsset{compat=1.18}\"]}'}\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\nConsider a normally distributed nationwide test.\n\nThe total shaded area between the curve and the straight\nhorizontal line can be thought of as one hundred percent\nof that area. In the world of probability, we measure\nthat area as 1. The curve is symmetrical, so measure all\nthe area to the left of the highest point on the curve\nas 0.5. That is half, or fifty percent, of the total\narea between the curve and the horizontal line at the\nbottom. Instead of saying *area between the curve and\nthe horizontal line at the bottom*, people usually say\n*the area under the curve*.\n\nFor any value along the $x$-axis, the $y$-value on the\ncurve represents the value of the probability density\nfunction.\n\nThe area bounded by the vertical line between the\n$x$-axis and the corresponding $y$-value on the curve,\nthough, is what we are usually interested in because\nthat area represents probability.\n\nHere is a graph of the *size* of that area. It's called the\ncumulative distribution function.\n\n\n::: {.cell engine.opts='{\"extra.preamble\":[\"\\\\usepackage{pgfplots,stix,pagecolor}\",\"\\\\pgfplotsset{compat=1.18}\"]}'}\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-19-1.png){width=50%}\n:::\n:::\n\n\nThe above graph can be read as having an input and\noutput that correspond to the previous graph of the\nprobability density function. As we move from right to\nleft on the $x$-axis, the area that would be to the left of\na given point on the probability density function is the\n$y$-value on this graph. For example, if we go half way\nacross the $x$-axis of the probability density function,\nthe area to its left is one half of the total area, so\nthe $y$-value on the cumulative distribution function\ngraph is one half.\n\nThe shape of the cumulative distribution function is\ncalled a sigmoid curve. You can see how it gets this\nshape by looking again at the probability density\nfunction graph above. As you move from left to right on\nthat graph, the area under the curve increases very\nslowly, then more rapidly, then slowly again. The places\nwhere the area grows more rapidly and then more slowly\non the probability density function curve correspond to\nthe s-shaped bends on the cumulative distribution curve.\n\nAt the left side of the cumulative distribution curve,\nthe $y$-value is zero meaning zero probability.\nWhen we reach the right side of the cumulative distribution\ncurve, the $y$-value is 1 or 100 percent of the\nprobability.\n\nLet's get back to the example of a nationwide test.\nIf we say that students nationwide took an test that had\na mean score of 75 and that the score was normally\ndistributed, we're saying that the value on the $x$-axis\nin the center of the curve is 75. Moreover, we're saying\nthat the area to the left of 75 is one half of the total\narea. We're saying that the probability of a score less\nthan 75 is 0.5 or fifty percent. We're saying that half\nthe students got a score below 75 and half got a score\nabove 75.\n\nThat is called the frequentist\ninterpretation of probability. In general, that\ninterpretation says that a probability of 0.5 is\nproperly measured by saying that, if we could repeat the\nevent enough times, we would find the event happening\nhalf of those times.\n\nFurthermore, the frequentist interpretation of the\nnormal distribution is that, if we could collect enough\ndata, such as administering the above test to thousands\nof students, we would see that the graph of the frequency of\ntheir scores would look more and more like the bell curve\nin the picture, where $x$ is a test score and $y$ is the\nnumber of students receiving that score.\n\nSuppose we have the same test and the same distribution\nbut that the mean score is 60. Then 60 is in the middle\nand half the students are on each side. That is easy to\nmeasure. But what if, in either case, we would like to\nknow the probability associated with scores that are not\nat that convenient midpoint?\n\nIt's hard to measure any other area under the normal\ncurve except for $x$-values in the middle of the curve,\ncorresponding to one half of the area. Why is this?\n\nTo see why it's hard to measure the area corresponding\nto any value except the middle value, let's first\nconsider a different probability distribution, the\nuniform distribution. Suppose I have a machine that can\ngenerate any number between 0 and 1 at random. Further,\nsuppose that any such number is just as likely\nas any other such number.\n\nHere's a graph of the\nthe uniform distribution of numbers generated by the\nmachine.\nThe horizontal line is the probability density function\nand the shaded area is the cumulative distribution\nfunction from 0 to 1/2. In other words, the probability\nof the machine generating numbers from 0 to 1/2 is 1/2.\nThe probability of generating numbers from 0 to 1 is 1,\nthe area of the entire rectangle.\n\n\n::: {.cell engine.opts='{\"extra.preamble\":[\"\\\\usepackage{pgfplots,stix,pagecolor}\",\"\\\\pgfplotsset{compat=1.18}\"]}'}\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nIt's very easy to calculate any probability for this\ndistribution, in contrast to the normal distribution.\nThe reason it is easy is that you can just use the\nformula for the area of a rectangle, where area is base\ntimes side. The probability of being in the entire\nrectangle is $1\\times1=1$, and the probability of being\nin the part from $x=0$ to $x=1/4$ is just\n$1\\times(1/4)=1/4$.\n\nThe cumulative distribution function of the uniform\ndistribution is simpler than that of the normal\ndistribution because area is being added at the same\nrate as we move from left to right on the above graph.\nTherefore it is just a straight diagonal line from (0,1)\non the left to (1,1) on the right.\n\n\n::: {.cell engine.opts='{\"extra.preamble\":[\"\\\\usepackage{pgfplots,stix,pagecolor}\",\"\\\\pgfplotsset{compat=1.18}\"]}'}\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-21-1.png){width=50%}\n:::\n:::\n\n\nReading it is the same as reading the  cumulative\ndistribution function for the normal distribution. For\nany value on the $x$-axis, say, 1/2, go up to the\ndiagonal line and over to the value on the $y$-axis.\nIn this case, that value is 1/2.\nThat is the area under the horizontal line in the\nprobability density function graph from 0 to 1/2 (the\nshaded area).\nFor a rectangle, calculating area is trivial.\n\nCalculating the area of a curved region, like the normal\ndistribution, though, can be more difficult.\nIf you've studied any calculus, you know that\nthere are techniques for calculating the area under a\ncurve. These techniques are called integration\ntechniques. In the case of the normal distribution the\nformula for the height of the curve at any point on the\n$x$-axis is\n\\begin{equation*}\n\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-(x-\\mu)^2/2\\sigma^2}\n\\end{equation*}\nand the area is the integral of that quantity from\n$-\\infty$ to $x$, which can be rewritten as\n\\begin{equation*}\n\\frac{1}{\\sqrt{2\\pi}}\\int^x_{-\\infty}e^{-t^2/2}dt\n =(1/2)\\left(1+\\text{erf}\\left(\\frac{x-\\mu}{\\sigma\\sqrt{2}}\\right)\\right)\n\\end{equation*}\nThe integral on the left is difficult to evaluate\nso people use numerical approximation techniques to find\nthe expression on the right in the above equation.\nThose techniques are so time-consuming\nthat, rather than recompute them every time they are\nneeded, a very few people used to write the results into a\ntable and publish it and most people working with\nprobability would just consult the tables. Only in the\npast few decades have calculators become available\nthat can do the tedious approximations. Hence, most\nstatistics books, written by people who were educated\ndecades ago, still teach you how to use such tables.\nThere is some debate as to whether there is educational\nvalue in using the tables vs using calculators or\nsmartphone apps or web-based tables or apps. We'll\ndemonstrate them and assume that you use a\ncalculator or smartphone app on exams.\n\n### Using the normal distribution\n\nSince there is no convenient integration formula, people used\n  tables until recently.\nCurrently you can google tables or apps that do the work\n  of tables.\nWe're going to do two exercises with tables that help\ngive you an idea of what's going on. You can use your\ncalculator afterward. The main reason for what follows\nis so you understand the results produced by your\ncalculator to avoid ridiculous mistakes.\n\n### calculating z scores\n\n$$z=\\frac{y-\\mu}{\\sigma}$$\n\nCalculations use the fact that the *bell curve* is\nsymmetric and adds up to 1, so you can calculate one\nside and add it, subtract it, or double it\n\nFollowing are four examples.\n\n(a) $P(-1 \\leqslant z \\leqslant 1)$: Since the bell curve is\nsymmetric, find the area from $z=0$ to $z=1$ and double\nthat area. The table entry for $z=1$ gives the relevant\narea under the curve, .3413. Doubling this gives the\narea from -1 to 1: .6826. Using a calculator may give\nyou .6827 since a more accurate value than that provided\nby the table would be .6826895. This is an example where\nno points would be deducted from your score for using\neither answer.\n\n(b) $P(-1.96 \\leqslant z \\leqslant 1.96)$: This is one of the three\nmost common areas of interest in this course, the other\ntwo being the one in part (c) below and the one I will\nadd on after I show part (d) below. Here again, we can\nread the value from the table as .4750 and double it,\ngiving .95. This is really common because because 95\\%\nis the most commonly used confidence interval.\n\n(c) $P(-1.645 \\leqslant z \\leqslant 1.645)$: The table does not have\nan entry for this extremely commonly desired value. A\nstatistical calculator or software package will show\nthat the result is .45, which can be doubled to give\n.90, another of the three most frequently used\nconfidence intervals. If you use interpolation, you will\nget the correct answer in this case. Interpolation means\nto take the average of the two closest values, in this\ncase $(.4495+ .4505) / 2$. You will rarely, if ever need\nto use interpolation in real life because software has\nmade the tables obsolete and we only use them to try to\ndrive home the concept of $z$-scores relating to area\nunder the curve, rather than risking the possibility\nthat you learn to punch numbers into an app without\nunderstanding them. Our hope is that, by first learning\nthis method, you will be quick to recognize the results\nof mistakes, rather than naively reporting wacky results like\n*the probability is 1.5* just because you typed a wrong\nnumber.\n\n(d) $P(-3 \\leqslant z \\leqslant 3)$: The table gives .4987 and\ndoubling that gives .9974. A calculator would give the\nmore correct (but equally acceptable in this course)\nresult of .9973.\n\nThe other common confidence interval I mentioned above\nis the 99\\% confidence interval, used in cases\nwhere the calculation relates to something\nlife-threatening, such as a question involving a\npotentially life-saving drug or surgery. \nA table or calculator will show that the\n$z$-score that would lead to this result is 2.576. So if\nyou were asked to compute $P(-2.576 \\leqslant z \\leqslant 2.576)$,\nthe correct answer would be .99 or 99\\%. To use a\ncalculator or statistical app to find the $z$-score\ngiven the desired probability, you would look in an app for\nsomething called a quantile function.\n\n### sketch the graphs of probabilities\n\nSketch the normal curve six\ntimes, identifying a different region on it each time.\nFor these graphs, let $y\\sim N(100,8)$.\n\n\n::: {.cell engine.opts='{\"extra.preamble\":[\"\\\\usepackage{pgfplots,stix,pagecolor}\",\"\\\\pgfplotsset{compat=1.18}\"]}'}\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n::: {.cell engine.opts='{\"extra.preamble\":[\"\\\\usepackage{pgfplots,stix,pagecolor}\",\"\\\\pgfplotsset{compat=1.18}\"]}'}\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n::: {.cell engine.opts='{\"extra.preamble\":[\"\\\\usepackage{pgfplots,stix,pagecolor}\",\"\\\\pgfplotsset{compat=1.18}\"]}'}\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n::: {.cell engine.opts='{\"extra.preamble\":[\"\\\\usepackage{pgfplots,stix,pagecolor}\",\"\\\\pgfplotsset{compat=1.18}\"]}'}\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n::: {.cell engine.opts='{\"extra.preamble\":[\"\\\\usepackage{pgfplots,stix,pagecolor}\",\"\\\\pgfplotsset{compat=1.18}\"]}'}\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n::: {.cell engine.opts='{\"extra.preamble\":[\"\\\\usepackage{pgfplots,stix,pagecolor}\",\"\\\\pgfplotsset{compat=1.18}\"]}'}\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\n## central limit theorem\n\n### Here's the definition of the central limit theorem.\n\nFor large sample sizes, the sample mean $\\overline{y}$\nfrom a population with mean $\\mu$ and standard deviation\n$\\sigma$ has a sampling distribution that is\napproximately normal, regardless of the probability\ndistribution of the sampled population.\n\n### Why care about the central limit theorem?\n\nIn business, distributions of phenomena like waiting\ntimes and customer choices from a catalog are typically\nnot normally distributed, but instead long-tailed. The\ncentral limit theorem means that\nresampling the mean of any of these distributions can be\ndone on a large scale using the normal distribution\nassumptions without regard to the underlying\ndistribution. This simplifies many real-life\ncalculations. For instance, waiting times at each bus\nstop are exponentially distributed but if we take the\nmean waiting time at each of 100 bus stops, the mean of\nthose 100 times is normally distributed, even though the\nindividual waiting times are drawn from an exponential\ndistribution.\n\n<!---\n% The following function takes two parameters, mean and\n% standard deviation, so it's called, for instance, by\n% saying\n%    gauss(6.5,1)\n% to specify a gaussian distribution with mean 6.5 and\n% standard deviation 1.\n%\n--->\n\n<!---\n% PGFPlots notes:\n%\n% The parameter\n%    no markers\n% when applied to an entire axis disables all markers.\n%\n% The parameters\n%    domain=0:10, samples=100\n% cause the gauss function to be sampled 100 times in\n% that interval - doesn't control the display.\n%\n% The parameter\n%    axis lines*=left\n% includes a * that causes no arrow heads on axis,\n% according to the pgfplots manual, p. 218.\n%\n% The directive\n%    \\closedcycle\n% at the end of the filled plot is necessary for any\n% filled area and completes a path from the last point\n% back to the first point in the path.\n%\n--->\n\n### normal distribution example problem\n\nConsider an example for calculating a\n$z$-score where $x\\sim N(50,15)$, which is a statistical\nnotation for saying $x$ is a random normal variable with mean\n50 and standard deviation 15. It is also read as if you\nsaid that $x$ has the normal distribution with mean 50\nand standard deviation 15.\n\n#### Picture the example.\n\n\n::: {.cell engine.opts='{\"extra.preamble\":[\"\\\\usepackage{pgfplots,stix,pagecolor}\",\"\\\\pgfplotsset{compat=1.18}\"]}'}\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-28-1.png){width=50%}\n:::\n:::\n\n\n#### Here's what we input into the $z$ calculation.\n\nIn order to identify the size of the shaded area, we can\nuse the table of $z$-scores by standardizing the\nparameters we believe to apply,\nas if they were the\npopulation parameters $\\mu$ and $\\sigma$. We only do\nthis if we have such a large sample that we have reason\nto believe that the sample values approach the\npopulation parameters. For the much, much more typical\ncase where we have a limited amount of data, we'll learn\na more advanced technique that you will use much more\nfrequently in practice.\n\n#### We input a $z$ to get an output probability.\n\nThe table in our textbook contains an input in the\nleft and top margins and an output in the body. The\ninput is a $z$-score, the result of the calculation\n$$z=\\frac{y-\\mu}{\\sigma}$$\nwhere $z\\geq 0$. The output is a number in the body\nof the table, expressing the probability\nfor the area\nbetween the normal curve and the axis, from the mean (0) to\n$z$. Note that the values of $z$ start at the mean and\ngrow toward the right end of the graph. If $z$ were\n$\\infty$, the shaded area would be 0.5, also known as 50\npercent.\n\n#### This is the $z$-score table concept.\n\n\n::: {.cell engine.opts='{\"extra.preamble\":[\"\\\\usepackage{pgfplots,stix,pagecolor}\",\"\\\\pgfplotsset{compat=1.18}\"]}'}\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-29-1.png){width=50%}\n:::\n:::\n\n\n#### Calculate the $z$-score to input.\n\nFor now, let's calculate the $z$-score as\n$$z=\\frac{y-\\mu}{\\sigma}=\\frac{70-50}{15}=1.33$$\ngiving half of the answer we're seeking:\n\n## Apply the $z$-score to the table.\n\n\n::: {.cell engine.opts='{\"extra.preamble\":[\"\\\\usepackage{pgfplots,stix,pagecolor}\",\"\\\\pgfplotsset{compat=1.18}\"]}'}\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-30-1.png){width=50%}\n:::\n:::\n\n\n#### Obtain an intermediate result.\n\nNow use this to read the table. The input is 1.33 and\nyou'll use the left and top margins to find it. The\noutput is the corresponding entry in the body of the\ntable, .4082, also known as 40.82 percent of the area under the\ncurve.\n\n#### Finish the example.\n\nRecall that our initial problem was to find $P(30<y<70)$\nand what we've just found, .4082, is $P(50<y<70)$. We\nmust multiply this result by 2 to obtain the correct\nanswer, .8164 or 81.64 percent. That is to say that the\nprobability that $y$ is somewhere between 30 and 70 is\n.8164 or 81.64 percent. As a reality check, all\nprobabilities for any single event must sum to 1, and\nthe total area under the curve is 1, so it\nis a relief to note that the answer we've found is less\nthan 1. It's also comforting to note that the shaded\narea in the original picture of the example looks like\nit could plausibly represent about 80 percent of the\ntotal area. It is easy to get lost in the mechanics of\ncalculations and come up with a wildly incorrect answer\nbecause of a simple arithmetic error.\n\n### Some $z$-score tables differ from ours.\n\nBear in mind that anyone can publish a $z$-score table\nusing their own customs. Students have found such\ntables that define $z$ as starting at the extreme left\nof the curve. If we used such a table for the above\nexample, the output would have been .9082 instead of\n.4082 and we would have had to subtract the left side,\n.5, from that result before multiplying by 2.\n\n### typical exam questions for $z$-scores\n\nMany exam-style problems will ask questions such that\nyou must do more or less arithmetic with the result from the\ntable. Consider these questions, still using the above\nexample where $y\\sim N(50,15)$:\n\n- What is the probability that $y$ is greater than 50?\n- What is the probability that $y$ is greater than 70?\n- What is the probability that $y$ is less than 30?\n- What is the probability that $y$ is between 30 and 50?\n\n#### Answer the preceding questions.\n\nEach of these questions can be answered using $z=1.33$\nexcept the first. Since we know that $y$ is normally\ndistributed, we also know that the probability of $y$\nbeing greater than its mean is one half, so the answer\nto the first question is 0.5 or fifty percent.\nThe second question simply requires us to subtract the\nresult from the table, .4082, from .5 to find the area\nto the right of 1.33, which is .0918 or 9.18 percent.\nThe third question is symmetrical with the second, so we\ncan just use the method from the second question to find\nthat it is also .0918. Similarly, the fourth question is\nsymmetrical with the first step from the book example,\nso the answer is the answer to that first step, .4082.\n\n#### This one takes an extra step.\n\nWhat is the probability that $y$ is between 30 and 40?\n\n\n::: {.cell engine.opts='{\"extra.preamble\":[\"\\\\usepackage{pgfplots,stix,pagecolor}\",\"\\\\pgfplotsset{compat=1.18}\"]}'}\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-31-1.png){width=50%}\n:::\n:::\n\n\n#### Find the difference between areas.\n\nSubtract the probability that $y$ is between 50 and 60\nfrom the probability that $y$ is between 50 and 70.\n\n\n::: {.cell engine.opts='{\"extra.preamble\":[\"\\\\usepackage{pgfplots,stix,pagecolor}\",\"\\\\pgfplotsset{compat=1.18}\"]}'}\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-32-1.png){width=50%}\n:::\n:::\n\n\n### How do you find areas in $z$-score tables?\n\n- draw picture to help you understand the question\n- standardize the picture so you can use a table or a\n- draw the standardized picture\n- pick one of three kinds of tables / apps\n- write the standardized result (may do this multiple times)\n- fit the standardized result(s) into the original problem\n\nLet's look at these steps with an example.\nSuppose that $y\\sim N(50,8)$. In words, this means that\n$y$ has the normal distribution with true mean 50 and\ntrue standard deviation 8. Let's answer the question\n*What's the probability that $y>40$?*\n\nStep 1 is to draw a picture to make sense of the\nquestion. The picture shows the area under the curve\nwhere the scale marking is to the right of 40. This\npicture tells you right away that the number that will\nanswer the question is less than 1 (the entire curve\nwould be shaded if it were 1) and more than 1/2 (the\nportion to the right of 50 would be 1/2 and we have\ncertainly shaded more than that).\n\n\\begin{tikzpicture}\n\\begin{axis}[\n  no markers, domain=20:80, samples=100,\n  axis lines=left, xlabel=$y$, ylabel=$f(y)$,\n  every axis y label/.style={at=(current axis.above origin),anchor=south},\n  every axis x label/.style={at=(current axis.right of origin),anchor=west},\n  height=5cm, width=12cm,\n  xtick={40,50}, ytick=\\empty,\n  enlargelimits=false, clip=false, axis on top,\n  %grid = major\n  ]\n  \\addplot [fill=cyan!20, draw=none, domain=40:80] {gauss(50,8)} \\closedcycle;\n  \\addplot [very thick,cyan!50!black] {gauss(50,8)};\n\\end{axis}\n\\end{tikzpicture}\n\nStep 2 is to standardize the question so we can use a\ntable or app to find the probability / area. We use the\nequation $z=(y-\\mu)/\\sigma$ with values from the\noriginal question: $(50-40)/8=-10/8=-1.25$. Now we know the labels\nthat would go on a standardized picture similar to the\npicture above.\nNow we can ask the standardized\n question\n*What's the probability that $z>-1.25$?*\n\nStep 3 is to draw that standardized picture. It's the\nsame as the picture above except standardized so that it\nfits the tables / apps for calculating probability /\narea. Now, instead of $y$ we're looking for $z$ and the\nprobability associated with $z$ on a standardized table\nwill be the same as for $y$ on a table for the\nparameters given in the original question.\n\n\\begin{tikzpicture}\n\\begin{axis}[\n  no markers, domain=-4:4, samples=100,\n  axis lines=left, xlabel=$z$, ylabel=$f(z)$,\n  every axis y label/.style={at=(current axis.north west),anchor=south},\n  every axis x label/.style={at=(current axis.right of origin),anchor=west},\n  height=5cm, width=12cm,\n  xtick={-1.25,0}, ytick=\\empty,\n  enlargelimits=false, clip=false, axis on top,\n  %grid = major\n  ]\n  \\addplot [fill=cyan!20, draw=none, domain=-1.25:4] {gauss(0,1)} \\closedcycle;\n  \\addplot [very thick,cyan!50!black] {gauss(0,1)};\n\\end{axis}\n\\end{tikzpicture}\n\nStep 4 is to pick one of the three kinds of tables /\napps to input the standardized $z$ score to get a\nprobability as output. In this example, we only have to\ndo this step once because we only want to know the area\ngreater than $y$. If we wanted to know a range between\ntwo $y$ values, we'd need the $z$ scores for each of\nthem so we'd have to do it twice.\n\nThe three kinds of output from tables / apps are as follows.\n\n\n::: {.cell engine.opts='{\"extra.preamble\":[\"\\\\usepackage{pgfplots,stix,pagecolor}\",\"\\\\pgfplotsset{compat=1.18}\"]}'}\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-33-1.png){width=50%}\n:::\n:::\n\n::: {.cell engine.opts='{\"extra.preamble\":[\"\\\\usepackage{pgfplots,stix,pagecolor}\",\"\\\\pgfplotsset{compat=1.18}\"]}'}\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-34-1.png){width=50%}\n:::\n:::\n\n::: {.cell engine.opts='{\"extra.preamble\":[\"\\\\usepackage{pgfplots,stix,pagecolor}\",\"\\\\pgfplotsset{compat=1.18}\"]}'}\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-35-1.png){width=50%}\n:::\n:::\n\n\nThe first figure shows how the table works in some books.\nIt provides the value from 0 to $|z|$.\nIf $z$ is negative, using this table requires you to\ninput\n$|z|$ instead. In this question, the value you get will\nbe .3944. To get the ultimate answer to this question,\nStep 5 will be to add this value to .5, giving a final\nanswer of .8944.\n\nThe second figure shows how most tables and apps work.\nIt gives the value from $-\\infty$ to $z$. In this\nquestion, the value you get will be .1056. To get the\nultimate answer to this question, Step 5 will be to\nsubtract\nthis value from 1, giving a final answer of .8944.\n\nThe bottom figure shows how some tables and apps work.\nIt gives the value from z to $+\\infty$. In this\nquestion, the value you get will be .8944. To get the\nultimate answer to this question, Step 5 will be to\nsimply report this value,\ngiving a final answer of .8944.\n\nNotice that all three types of tables / apps lead to the\nsame result by different paths. In this case, the right\nfigure is the most convenient but, for other questions,\none of the others may be more convenient.\n\nStep 5, the final step is to use the value you got from\na table or app in conjunction with the original picture\nyou drew in Step 1. Since the procedure for step 5\ndepends on the table / app you use, I gave the procedure\nfor Step 5 above in the paragraphs for top, middle, and\nbottom figure.\n\n## estimate a population parameter\n\n### Use a large-sample confidence interval.\n\nA large-sample $100(1-\\alpha)\\%$ confidence interval for\na population mean, $\\mu$, is given by\n\n$$\\mu \\pm z_{\\alpha/2} \\sigma_y \\approx \\overline{y}\\pm z_{\\alpha/2} \\frac{s}{\\sqrt{n}}$$\n\n## elements of a hypothesis test\n\n- Null hypothesis\n- Alternative hypothesis\n- Test statistic\n- Level of significance\n- Rejection region marker\n- $p$-value\n- Conclusion\n\n### Work out a standard deviation example.\n\nExample exam question: ten samples of gas mileage have been\ncalculated for a new model of car by driving ten\nexamples of the car each for a week.\nThe average of the ten samples is 17. The sum of\nthe squares of the sample is 3281. What is their\nstandard deviation?\n\n#### Recognize appropriate formulas.\n\nThe information in the problem statement hints that you\nshould use\n$$s=\\sqrt{\\frac{\\sum_{i=1}^n y_i^2 -\nn(\\overline{y})^2}{n-1}}$$\nso you write\n$$s=\\sqrt{\\frac{3281 - 10(17)^2}{10-1}}=\\sqrt{\\frac{3281-2890}{9}}=\\sqrt{43.4444}=6.5912$$\n\n#### Why was the subtraction result positive?\n\nThe sample of ten gas mileage estimates is 17, 18, 16,\n20, 14, 17, 21, 13, 22, 12, 17. The sum of their squares\nis inevitably larger than or equal to the mean squared times the\nnumber of values. The easiest way to see this is to use\na series of identical values. Hence, finding the sum of the\nsquares is the same as calculating the mean squared\ntimes the number of values. There is no variance at all\nin such a sample, so it makes sense to arrive at a\nstandard deviation of zero. Is there any way to alter\nsuch a sample so that the sum of the squared values\nfalls below the mean? No.\n\n#### What should you do when you don't understand?\n\nThe previous example was developed as an answer to the\nquestion, *what do I do if I need to do a negative\nsquare root?* You can figure out that you will never\nneed to do so by the preceding process of finding a way\nto make $\\sum y_i^2 = n(\\overline{y}^2)$ and then trying\nto alter the values to decrease the left side or\nincrease the right side.\n\n## Make an inference about a population parameter.\n\nAt the beginning of the course, I said that statistics\ndescribes data and makes inferences about data. This\ncourse is partly about the latter, making inferences. You can\nmake two kinds of inferences about a population\nparameter: estimate it or test a hypothesis about it.\n\n### First distinguish between small and large.\n\nYou may have a small sample or a large sample. The\ndifference in the textbook is typically given as a\ncutoff of 30. Less is small, more is large. Other\ncutoffs are given, but this is the most prevalent.\n\nLarge samples with a normal distribution can be\nused to estimate a population mean using a $z$-score.\nSmall samples can be used to estimate a population mean using a $t$-statistic.\n\n### Sampling leads to a new statistic.\n\nIf we take more and more samples from a given\npopulation, the variability of the samples will\ndecrease. This relationship gives rise to the standard\nerror of an estimate\n$$\\sigma_{\\overline{y}}=\\frac{\\sigma}{\\sqrt{n}}$$\n\nThe standard error of the estimate is not exactly the\nstandard deviation. It is the standard deviation divided\nby a function of the sample size and it shrinks as the\nsample size grows.\n\n## estimate a population mean\n\n### Find a confidence interval.\n\nIf the sample is larger than or equal to 30, use the\nformula for finding a large-sample confidence interval\nto estimate the mean.\n\nA large-sample $100(1-\\alpha)\\%$ confidence interval for\na population mean, $\\mu$, is given by\n\n$$\\mu \\pm z_{\\alpha/2} \\sigma_{\\overline{y}} \\approx \\overline{y}\\pm z_{\\alpha/2} \\frac{s}{\\sqrt{n}}$$\n\n### For a small sample, use $t$.\n\nIf the sample is smaller than 30, calculate a\n$t$-statistic and use the\nformula for finding a small-sample confidence interval\nto estimate the mean.\n\nThe $t$-statistic you calculate from the small sample\nis\n$$\nt=\\frac{\\overline{y}-\\mu_0}{s/\\sqrt{n}}\n$$\n\nDoes your calculated $t$ fit within a\n$100(1-\\alpha)\\%$ confidence interval?\nFind out by calculating that interval.\nA small-sample $100(1-\\alpha)\\%$ confidence interval for\na population mean, $\\mu$, is given by\n\n$$\\mu \\pm t_{\\alpha/2} s_{\\overline{y}} \\approx\n\\overline{y}\\pm t_{\\alpha/2,\\nu} \\frac{s}{\\sqrt{n}}$$\nHere, you are using a $t$ statistic based on a chosen\n$\\alpha$ to compare to your calculated $t$-statistic.\nThe Greek letter $\\nu$, pronounced nyoo, represents the\nnumber of degrees of freedom.\n\n### Find a probability to estimate a mean.\n\nEstimating a mean involves finding a region where the\nmean is likely to be. The first question to ask is *how\nlikely?* and to do so, we use a concept called alpha,\ndenoted by the Greek letter $\\alpha$. Traditionally,\npeople have used three values of $\\alpha$ for the past\ncentury, .10, .05, and .01. These correspond to regions\nof $1-\\alpha$ under the normal distribution curve, so\nthese regions are 90 percent of the area, 95 percent of\nthe area, and 99 percent of the area. What we are\nsaying, for instance, if $\\alpha=0.01$ is that we are 99\npercent confident that the true population mean lies\nwithin the region we've calculated, $\\overline{y}\\pm\n2.576\\sigma_{\\overline{y}}$\n\n### $\\alpha$ selection is driven by criticality.\n\nTraditionally, $\\alpha=0.01$ is used in cases where life\ncould be threatened by failure.\n\nTraditionally, $\\alpha=0.05$ is used in cases where\nmoney is riding on the outcome.\n\nTraditionally, $\\alpha=0.10$ is used in cases where\nthe consequences of failing to capture the true mean are\nnot severe.\n\n![](fiSignifLevels.jpg)\n\nThe above picture shows these three cases. The top\nversion, *life-or-death*, has the smallest rejection\nregion. Suppose the test is whether a radical cancer\ntreatment gives longer life than a traditional cancer\ntreatment. Let's say that the traditional treatment\ngives an average of 15 months longer life. The null\nhypothesis is that the new treatment also gives 15\nmonths longer life. The alternative hypothesis is that\nthe radical treatment gives 22 months more life on\naverage based on on only five patients who received the\nnew treatment. A patient can only do one treatment or the\nother. The status quo would be to take the traditional\ntreatment unless there is strong evidence that the\nradical treatment provides an average longer life. In\nthe following picture, the shaded area is where the test\nstatistic would have to fall for us to say that there is\nstrong evidence that the radical treatment provides\nlonger life. We want to make that shaded region as small\nas possible so we minimize the chance our test statistic\nlands in it by mistake.\n\nWe can afford to let that shaded area be bigger\n(increasing the chance of mistakenly landing in it) if\nonly money, not life, is riding on the outcome. And we\ncan afford to let it be bigger still if the consequences\nof the mistake are small. To choose an $\\alpha$ level,\nask yourself how severe are the consequences of a Type I\nerror.\n\n### practice estimating a population mean\n\nConsider an outlet store that\npurchased used display panels to refurbish and resell.\nThe relevant statistics for failure time of the sample are\n$n=50, \\overline{y}=1.9350, s=0.92865$.\n\nBut these are the only necessary numbers\nto solve the problem. The standard error, .1313 can be calculated from\n$s/\\sqrt{n} = 0.92865/\\sqrt{50}=.1313.$\n\nFirst, find the 95\\% confidence\ninterval, which\ncan be calculated from the above information. Since the\nsample is greater than or equal to 30, we use the large\nsample formula:\n\\begin{align*}\n\\overline{y}\\pm z_{\\alpha/2} \\sigma_{\\overline{y}} \\approx \\overline{y}\\pm z_{\\alpha/2} \\frac{s}{\\sqrt{n}}\n &= 1.9350 \\pm 1.96 (.1313) \\\\\n &= 1.9350 \\pm 0.2573 \\\\\n &= (1.6777,2.1932)\n\\end{align*}\nThe correct answer to a question asking for the confidence interval is simply that pair of\nnumbers, 1.6777 and 2.1932.\n\nNow interpret that.\nThis result says that we are 95\\% confident that the\ntrue average time to failure for these panels is\nsomewhere between 1.6777 years and 2.1932 years.\nIt is tempting to rephrase the result. Be careful that you\ndon't say something with a different meaning.\nSuppose the store wants to offer a\nwarranty on these panels.\nKnowing that we are 95\\% confident that the true mean is in the\ngiven range helps the store evaluate the risk of\ndifferent warranty lengths. The correct answer is to say\nthat we are 95\\% confident that the true mean\ntime to failure for these panels is\nsomewhere between 1.6777 years and 2.1932 years.\n\nThe\nmeaning of the 95 percent confidence interval is that,\nif we repeatedly resample the population, computing a\n95\\% confidence interval for each sample, we expect 95\\%\nof the confidence intervals generated to capture the\ntrue mean.\n\nAny statistics software can also offer a graphical\ninterpretation, such as a stem-and-leaf plot or\nhistogram.\nThe stem-and-leaf plot uses the metaphor of a stem\nbearing some number of leaves. In the following\nstem-and-leaf plot, the stem represents the first digit\nof a two-digit number. The top row of the plot has the\nstem 0 and two leaves, 0 and 2. Each leaf represents a\ndata point as the second digit of a two-digit number.\nIf you count the leaves (the digits to the\nright of the vertical bar), you will see that there are\nfifty of them, one for each recorded failure time. You\ncan think of each stem as holding all the leaves in a\ncertain range. The top stem holds all the leaves in the\nrange .00 to .04 and there are two of them. The next\nstem holds the six leaves in the range .05 to .09.\nThe third stem holds all six\nleaves in the range .10 to .15.\nThe stem-and-leaf plot resembles a\nsideways bar chart and helps us see that the\ndistribution of the failure times is somewhat\nmound-shaped. The main advantages of the stem-and-leaf\nplot are that it is compact for the amount of\ninformation it conveys and that it does not require a\ngraphics program or even a computer to quickly construct\nit from the raw data. The programming website\n[http://rosettacode.org](http://rosettacode.org)\nuses the stem-and-leaf plot as a programming task,\ndemonstrating how to create one in 37 different\nprogramming languages.\n\n    0 | 02\n    0 | 567788\n    1 | 122222\n    1 | 55666667888999\n    2 | 0223344\n    2 | 666788\n    3 | 00233\n    3 | 5555\n\nMost statistics programs offer many different histogram\ntypes. The simplest is equivalent to a barchart as\nfollows.\n\n![](fiFailureTime.png)\n\n## test a hypothesis about a population mean\n\n### Choose a null and alternative hypothesis.\n\nTesting a hypothesis must be done modestly. As an\nexample of what I mean by modestly, consider criminal\ntrials in the USA. The suspect on trial is considered\ninnocent until proven guilty. The more modest\nhypothesis (the null hypothesis) is that the person is innocent.\nThe more immodest hypothesis (the alternative\nhypothesis) carries the burden of proof.\n\n### Type I error is worse than Type II error.\n\nThe traditional view of the legal system in the USA is\nthat if you imprison an innocent person, it constitutes\na more serious error than letting a guilty person go\nfree.\n\nImprisoning an innocent person is like a Type I error:\nwe rejected the null hypothesis when it was true.\nLetting a guilty person go free is like a Type II error:\nwe failed to reject the null hypothesis when it was\nfalse.\n\n### identify rejection region marker\n\nHow do you identify rejection\nregion markers? A rejection region marker is the value a\ntest statistic that leads to\nrejection of the null hypothesis.\nIt marks the edge of a region under the curve\ncorresponding to the level of significance, $\\alpha$.\nThe marker is not a measure of the size of the region.\nThe rejection region marker can vary from $-\\infty$ to\n$+\\infty$ while the size of the region is somewhere\nbetween zero and one.\n\nThe following table shows the relevant values of $\\alpha$ and related quantities.\n\n----------------------------------------------\n $1-\\alpha$ $\\alpha$ $\\alpha/2$ $z_{\\alpha/2}$\n ---------- -------- ---------- --------------\n    .90        .10      .05        1.645\n\n    .95        .05      .025       1.96\n\n    .99        .01      .005       2.576\n----------------------------------------------\n\n\nThe above table refers to two-tailed tests.\nOnly examples (e) and (f) below refer to\ntwo-tailed tests. In the other four cases, the\n$z$-scores refer to $z_{\\alpha}$ rather than\n$z_{\\alpha/2}$.\n\n\\(a\\) $\\alpha=0.025$, a one-tailed rejection region\n\n\n::: {.cell engine.opts='{\"extra.preamble\":[\"\\\\usepackage{pgfplots,stix,pagecolor}\",\"\\\\pgfplotsset{compat=1.18}\"]}'}\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-36-1.png){width=672}\n:::\n:::\n\n\n\\(b\\) $\\alpha=0.05$, a one-tailed rejection region\n\n\n::: {.cell engine.opts='{\"extra.preamble\":[\"\\\\usepackage{pgfplots,stix,pagecolor}\",\"\\\\pgfplotsset{compat=1.18}\"]}'}\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-37-1.png){width=672}\n:::\n:::\n\n\n\\(c\\) $\\alpha=0.005$, a one-tailed rejection region\n\n\n::: {.cell engine.opts='{\"extra.preamble\":[\"\\\\usepackage{pgfplots,stix,pagecolor}\",\"\\\\pgfplotsset{compat=1.18}\"]}'}\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-38-1.png){width=672}\n:::\n:::\n\n\n\\(d\\) $\\alpha=0.0985$, a one-tailed rejection region\n\n\n::: {.cell engine.opts='{\"extra.preamble\":[\"\\\\usepackage{pgfplots,stix,pagecolor}\",\"\\\\pgfplotsset{compat=1.18}\"]}'}\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-39-1.png){width=672}\n:::\n:::\n\n\n\\(e\\) $\\alpha=0.10$, a two-tailed rejection region\n\n\n::: {.cell engine.opts='{\"extra.preamble\":[\"\\\\usepackage{pgfplots,stix,pagecolor}\",\"\\\\pgfplotsset{compat=1.18}\"]}'}\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-40-1.png){width=672}\n:::\n:::\n\n\n\\(f\\) $\\alpha=0.01$, a two-tailed rejection region\n\n\n::: {.cell engine.opts='{\"extra.preamble\":[\"\\\\usepackage{pgfplots,stix,pagecolor}\",\"\\\\pgfplotsset{compat=1.18}\"]}'}\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-41-1.png){width=672}\n:::\n:::\n\n\n## test a hypothesis about a sample mean\n\nThe test statistic is\n$t_c$, where the $c$ subscript stands for *calculated*.\nMost people just call it $t$. In my mind, that leads\nto occasional confusion about whether it is a value\ncalculated from a sample.\nWe'll compare the test statistic to $t_{\\alpha,\\nu}$, the\nvalue of the statistic at a given level of\nsignificance, identified using a table or calculator.\n\nThe test leads to two situations. The first, pictured\nbelow, is the situation where we fail to reject the null\nhypothesis and conclude that we have not seen evidence\nin the sample that the point estimate differs from what the modest hypothesis claims it is. Often, this is an estimate of a mean, where the population mean, $\\mu_0$, is being estimated by a sample mean, $\\bar{y}$. The following picture doesn't show $\\bar{y}$, just the tail. $\\bar{y}$ would be at the top of the hump, not shown here.\n\n\n::: {.cell engine.opts='{\"extra.preamble\":[\"\\\\usepackage{pgfplots,stix,pagecolor}\",\"\\\\pgfplotsset{compat=1.18}\"]}'}\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-42-1.png){width=672}\n:::\n:::\n\n\nThe above possibility shows the situation where\n$t_{\\alpha,\\nu}>t_c$, which is equivalent to saying that\n$p>\\alpha$.\n\nThe $x$-axis scale begins to the left of the fragment shown here, and values of $t$\nincrease from left to right. At the same time, the\nshaded regions decrease in size from left to right.\nNote that the entire shaded region above is $p$,\nwhile only the darker region at the right is $\\alpha$.\n\nThe situation pictured below is the reverse. In this situation, we reject the null hypothesis and conclude instead that the point estimate does not equal the value in the modest hypothesis. In this case $t_{\\alpha,\\nu}<t_c$, which is equivalent to saying that $p<\\alpha$.\n\n\n::: {.cell engine.opts='{\"extra.preamble\":[\"\\\\usepackage{pgfplots,stix,pagecolor}\",\"\\\\pgfplotsset{compat=1.18}\"]}'}\n::: {.cell-output-display}\n![](week06_files/figure-html/unnamed-chunk-43-1.png){width=672}\n:::\n:::\n\n\nLet's make this more concrete with an example.\n\nSuppose that a bulk vending machine dispenses bags expected to contain 15 candies on average. The attendant who refills the machine claims it's out of whack, dispensing more than 15 candies on average, requiring more frequent replenishment and costing the company some extra money. The company representative asks him for a sample from the machine, so he produces five bags containing 25, 23, 21, 21, and 20 candies. Develop a hypothesis test to consider the possibility that the vending machine is defective. Use a level of significance that makes sense given the situation described above.\n\nThere are seven steps to solving this problem, as follows.\n\nStep 1. Choose a null hypothesis.\n\nAt a high level, we can say that we are trying to choose between two alternatives:\n\n- that the machine is defective, or\n- that the machine is operating normally.\n\nWe need to reduce this high level view to numbers. The problem states that the machine *is expected to* dispense 15 candies per bag, on average. This is equivalent to saying that the true mean is 15 or $\\mu=15$.\n\nIf the machine is defective in the way the attendant claims, then $\\mu>15$. So we could say that one of the hypotheses would be that the sample came from a population with $\\mu=15$ and the other hypothesis would be that the sample did not come from a population with $\\mu=15$. Which should be the null hypothesis?\n\nThe null hypothesis represents the status quo, what we would believe if we had no evidence either for or against. Do you believe that the machine is defective if there is no evidence either that it is or isn't? Let's put it another way. Suppose you arrested a person for a crime and then realized that you have no evidence that they did commit the crime and no evidence that they did not commit the crime. Would you imprison them or let them go free? If you let them go free, it means that your null hypothesis is that they are innocent unless proven guilty.\n\nThis suggests that if you have no evidence one way or the other, assume the machine is operating normally. We can translate this into the null hypothesis $\\mu=15$. The formal way of writing the null hypothesis is to say $H_0: \\mu=\\mu_0$, where $\\mu_0=15$. Later, when refer to this population mean, we call it $\\mu_0$ because it is the population mean associated with the hypothesis $H_0$. So later we will say $\\mu_0=15$.\n\nAt the end of step 1, we have chosen the null hypothesis:\n$H_0: \\mu=\\mu_0$ with $\\mu_0=15$.\n\nStep 2. Choose the alternative hypothesis. The appropriate alternative hypothesis can be selected from among three choices: $\\mu<\\mu_0$, $\\mu>\\mu_0$, or $\\mu \\ne \\mu_0$. The appropriate choice here seems obvious: all the sample values are much larger than $\\mu_0$, so if the mean we calculate differs from $\\mu_0$ it will have to be larger than $\\mu_0$. If all the values in our sample are larger than $\\mu_0$, there is just no way their average can be smaller than $\\mu_0$.\n\nAt the end of step 2, we have determined the alternative hypothesis to be\n\n$H_a: \\mu>\\mu_0$ with $\\mu_0=15$.\n\nStep 3. Choose the test statistic. Previously, we have learned two test statistics, $z$ and $t$. We have learned that the choice between them is predicated on sample size. If $n\\geqslant30$, use $z$, otherwise use $t$. Here $n=5$ so use $t$. We can calculate the $t$-statistic, which I called $t_c$ for *calculated* above, for the sample using the formula\n$$\nt=\\frac{\\overline{y}-\\mu_0}{s/\\sqrt{n}}\n$$\n\nWe can calculate the values to use from the following formulas or by using a machine.\n\n$$\\overline{y}=\\sum{y_i}/n=22$$\n\n$$s=\\sqrt{\\frac{\\sum y_i^2 -\nn(\\overline{y})^2}{n-1}}=2$$\n\n$\\mu_0$ was given to us in\nthe problem statement and $\\sqrt{n}$ can be determined\nwith the use of a calculator or spreadsheet program.\nThe calculated $t$-statistic is\n$t_c=(22-15)/(2/\\sqrt{5})=7.8262$.\n\nAt the end of step 3, you have determined and calculated\nthe test statistic, $t_c=7.8262$.\n\nStep 4. Determine the level of significance, $\\alpha$. You choose the appropriate value of $\\alpha$ from the circumstances given in the problem statement. Previously in class, I claimed that there are three common levels of significance in use as sumarized in the table on page 35: 0.01, 0.05, and 0.10. I gave rules of thumb for these three as 0.01 *life-or-death*, 0.05 *money is riding on it*, and 0.10 *casual / low budget*. In this case, the consequences seem to be a small amount of money lost by the company if they are basically giving away candies for free. I suggest that this is a case where some *money is riding on it* so choose $\\alpha=0.05$.\n\nAt the end of step 4, you have determined $\\alpha=0.05$.\n\nStep 5. Identify the rejection region marker. This is simply a\nmatter of calculating (or reading from a table) an\nappropriate $t$-statistic for the $\\alpha$ you chose in\nthe previous step. This is\n$t_{\\alpha,\\nu}=t_{0.05,4}=2.131847$. Note that $\\nu$ is\nthe symbol the book uses for df, or degrees of freedom.\nIt is a Greek letter pronounced nyoo.\nFor a single sample $t$-statistic, df$=\\nu=n-1$.\n\nThis can be found using R by saying\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqt(0.95,4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.131847\n```\n:::\n:::\n\n\nAt the end of step 5, you have calculated the location\nof the rejection region (but not its size). It is\nlocated everywhere between the $t$ curve and the\nhorizontal line to the right of the point\n$t=2.131847$.\n\nStep 6. Calculate the $p$-value. This is the size of the\nregion whose location was specified in the previous\nstep, written as $p=P(t_{\\alpha,\\nu}>t_c)$.\nIt is the probability of observing a $t$-statistic\ngreater than the calculated $t$-statistic if the null\nhypothesis is true. It is found by a calculator or app\nor software. It can only be calculated by hand if you\nknow quite a bit more math than is required for this\ncourse. In this case\n$p=P(t_{\\alpha,\\nu}>t_c)=0.0007195$.\n\nWe can identify both $t_\\alpha$ and $t_c$ using R as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#. t sub alpha\nqt(0.95,4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.131847\n```\n:::\n\n```{.r .cell-code}\n#. find the alpha region associated with t sub alpha\n1-pt(2.131847,4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.04999999\n```\n:::\n\n```{.r .cell-code}\nx <- c(25,23,21,21,20)\nt.test(x,alternative=\"greater\",mu=15)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tOne Sample t-test\n\ndata:  x\nt = 7.8262, df = 4, p-value = 0.0007195\nalternative hypothesis: true mean is greater than 15\n95 percent confidence interval:\n 20.09322      Inf\nsample estimates:\nmean of x \n       22 \n```\n:::\n:::\n\n\nAlso note that the $t$ test tells us that the true mean is far from 15. If we tested repeatedly, we would find that it is greater than 20.09322 in 95 percent of the tests.\n\nAt the end of step 6, we have calculated the $p$-value,\n$p=P(t_{\\alpha,\\nu} > t_c)=0.0007195$.\n\nStep 7. Form a conclusion from the hypothesis test. We reject the null hypothesis that $\\mu=\\mu_0$, or in other words, we reject the hypothesis that these five bags came from a vending machine that dispenses an average of 15 candies per bag. Notice we don't that the machine is defective. Maybe the data were miscounted. We don't know. We have to limit our conclusion to what we know about the data presented to us, which is that the data presented to us did not come from a machine that dispense an average of 15 candies per bag.\n\nTo summarize the answer, the seven elements of this statistical test of hypothesis are:\n\n1. __null hypothesis__ $H_0: \\mu=15$\n2. __alternative hypothesis__ $H_{a}: \\mu>15$\n3. __test statistic__ $t_c=7.8262$\n4. __level of significance__ $\\alpha=0.05$\n5. __rejection region marker__ $t_{0.05,4} = 2.131847$\n6. __$p$-value__ $P(t_{\\alpha,\\nu}>t_c)=0.0007195$\n7. __conclusion__ Reject the null hypothesis that these\nfive bags came from a machine that dispenses an average\nof 15 candies per bag.\n\nLet's return to the two pictures we started with. Notice that $p<\\alpha$ in this case, which is equivalent to saying that $t_c>t_{\\alpha,\\nu}$, so we are looking at the second of the two pictures.\n\n",
    "supporting": [
      "week06_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}