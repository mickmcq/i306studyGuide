{"title":"More about Inference","markdown":{"yaml":{"title":"More about Inference"},"headingText":"Recap Week 06","containsRefs":false,"markdown":"\n\n```{r, include=FALSE}\nknitr::opts_chunk$set(\n  message=FALSE\n)\nlibrary(tidyverse)\n```\n\n\n- ggplot2\n- Foundations for Inference\n  - Point estimates\n  - Confidence Intervals\n  - Hypothesis Tests\n\n### A useful ggplot trick\n\nSome of you have struggled to produce boxplots of variables like `price` and `odometer` because they have ridiculous outliers. One way to overcome this that we've explored is to remove the extreme observations. A possibly better way, though, is illustrated below. Saying `outlier.shape=NA` in the `geom_boxplot()` function removes outliers before the dimensions of the plot are calculated. The result is a much more readable boxplot containing most of the cars.\n\n```{r}\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/vehicles.Rdata\"))\nlibrary(tidyverse)\ndf |> ggplot(aes(condition,odometer))+geom_boxplot(outlier.shape=NA)+  scale_y_continuous(limits = quantile(df$odometer, c(0.1, 0.8),na.rm=TRUE))\ndf |> ggplot(aes(condition,price))+geom_boxplot(outlier.shape=NA)+  scale_y_continuous(limits = quantile(df$price, c(0.1, 0.8),na.rm=TRUE))\n```\n\nNote that, in the above example, I've stored `df` in `vehicles.Rdata` using the `save()` function. Before saving it I converted `condition` to a factor and both `price` and `odometer` to numeric.\n\n### Preparing data\nNote that you can use the above quantile trick in other ways. For example, here I'm filtering out the extreme values of price, without necessarily knowing what they are.\n\n```{r}\ndf |>\n  filter(price >= quantile(price,.10) & price <= quantile(price,.90)) |>\n  count(state) |>\n  arrange(-n) |>\n  as_tibble() |>\n  print(n=12)\n```\n## Textbook section 6.1 Inference for a single proportion\n\n### Is the sample proportion nearly normal?\n\nNote: The book uses $p$ in two ways: as a $p$-value in a hypothesis test, and as $p$, a population proportion. Related to the population proportion is the sample proportion, $\\hat{p}$, pronounced p-hat. Too many $p$s!\n\nThe sampling distribution for $\\hat{p}$ based on a sample of size $n$ from a population with a true\nproportion $p$ is nearly normal when:\n\n1. The sample’s observations are independent, e.g., are from a simple random sample.\n2. We expected to see at least 10 successes and 10 failures in the sample, i.e., $np \\geqslant 10$ and $n(1 − p) \\geqslant 10$. This is called the success-failure condition.\n\nWhen these conditions are met, then the sampling distribution of $\\hat{p}$ is nearly normal with mean\n$p$ and standard error $\\text{SE}=\\sqrt{p(1-p)/n}$.\n\n### Confidence interval for a proportion\nA confidence interval provides a range of plausible values for the parameter $p$, and when $\\hat{p}$ can\nbe modeled using a normal distribution, the confidence interval for $p$ takes the form\n\n$$ \\hat{p} \\pm z^{*} \\times \\text{SE} $$\n\nwhere $z^{*}$ marks the $x$-axis for the selected confidence interval, e.g., 1.96 for a 95 percent confidence interval.\n\n### Prepare, Check, Calculate, Conclude Cycle\nThe OpenIntro Stats book recommends a four step cycle for both confidence intervals and hypothesis tests. It differs a bit from the seven step hypothesis testing method given in Week 06, but achieves the same result.\n\n- Prepare. Identify $\\hat{p}$ and $n$, and determine what confidence level you wish to use.\n- Check. Verify the conditions to ensure $\\hat{p}$ is nearly normal. For one-proportion confidence intervals, use $\\hat{p}$ in place of $p$ to check the success-failure condition.\n- Calculate. If the conditions hold, compute SE using $\\hat{p}$, find $z^{*}$, and construct the interval.\n- Conclude. Interpret the confidence interval in the context of the problem.\n\n### Same cycle for hypothesis testing for a proportion\n\n- Prepare. Identify the parameter of interest, list hypotheses, identify the significance level, and identify $\\hat{p}$ and $n$.\n- Check. Verify conditions to ensure $\\hat{p}$ is nearly normal under $H_0$. For one-proportion hypothesis tests, use the null value to check the success-failure condition.\n- Calculate. If the conditions hold, compute the standard error, again using $p_0$, compute the $Z$-score, and identify the $p$-value.\n- Conclude. Evaluate the hypothesis test by comparing the $p$-value to $\\alpha$, and provide a conclusion in the context of the problem.\n\n### Choosing sample size when estimating a proportion\nThis is probably the most important part of this section for practical purposes. The following expression denotes the *margin of error*:\n\n$$ z^{*}\\sqrt{\\frac{p(1-p)}{n}} $$\n\nYou have to choose the margin of error you want to report. The book gives an example of 0.04. So you want to find\n\n$$ z^{*}\\sqrt{\\frac{p(1-p)}{n}} < 0.04 $$\n\nThe problem is that you don't know $p$. Since the worst-case scenario is $p=0.5$, you have to use that unless you have some information about $p$. Recall that $z^{*}$ represents the $z$-score for the desired confidence level, so you have to choose that. The book gives an example where you want a 95 percent confidence level, so you choose 1.96. You could find this out in R by saying\n\n```{r}\nqnorm(0.025,lower.tail=FALSE)\n```\n\nreturning the $z$-score for the upper tail.\nThe reason for saying that 0.025 instead of 0.05 is that the probability of 0.05 is split between the tails.\nThe complementary function is `pnorm(1.959964,lower.tail=FALSE)`, which will return 0.025.\n\nOnce you have decided on values for $p$ and $z^*$, solve the above inequality for $n$.\n\n## Textbook section 6.2 Difference of two proportions\nIn this section, we're just modifying the previous section to account for a difference instead of a single proportion.\n\nThe difference $\\hat{p}_1-\\hat{p}_2$ can be modeled using the normal distribution when\n\n- The data are independent within and between the two groups (random samples or randomized experiment)\n- The success-failure condition holds for both groups (at least 10 successes and 10 failures in each sample)\n\n### Confidence intervals for $p_1-p_2$\n\n$$ \\text{point estimate } \\pm z^* \\times \\text{SE} \\rightarrow (\\hat{p}_1 - \\hat{p}_2) \\pm z^* \\times \\sqrt{\\frac{p_1(1-p_1)}{n_1}+\\frac{p_2(1-p_2)}{n_2}} $$\n\n### Hypothesis testing for $p_1-p_2$\n\nWhen the null hypothesis is that the proportions are equal, use the pooled proportion ($\\hat{p}_\\text{pooled}$)\nto verify the success-failure condition and estimate the standard error\n\n$$ \\hat{p}_\\text{pooled} = \\frac{\\text{number of “successes”}}{\\text{number of cases}} = \\frac{\\hat{p}_1n_1+\\hat{p}_2n_2}{n_1+n_2} $$\n\nHere $\\hat{p}_1n_1$ represents the number of successes in sample 1 since\n\n$$ \\hat{p}_1 = \\frac{\\text{number of successes in sample 1}}{n_1} $$\n\nSimilarly, $\\hat{p}_2n_2$ represents the number of successes in sample 2.\n\n## Textbook section 6.3 Testing goodness of fit using $\\chi^2$\n\nThe $\\chi^2$ test, pronounced k$\\overline{\\text{i}}$ square, is useful in many circumstances. The textbook treats two such circumstances:\n\n1. Suppose your sample can be divided into groups, as can the general population. Does your sample represent the general population?\n2. Does your sample resemble a particular distribution, such as the normal distribution?\n\nFor the first circumstance, we could divide a sample of people into races or genders and we would like to examine all at once for resemblance to the general population, rather than in pairs. The $\\chi^2$ statistic will permit an all-at-once comparison.\n\nThe $\\chi^2$ statistic is given by the following formula for $g$ groups.\n\n$$ \\chi^2 = \\frac{(\\text{observed count}_1-\\text{null count}_1)^2}{\\text{null count}_1} + \\cdots + \\frac{(\\text{observed count}_g-\\text{null count}_g)^2}{\\text{null count}_g} $$\n\nwhere the expression *null count* refers to the expected number of objects in the group. You have to be careful about how you determine the null count. For instance, the textbook gives an example of races of jurors. In such a case, the null counts should come from the population who can be selected as jurors. This might be a matter of some dispute since jurors are usually recruited through voting records and these records may not reflect the correct proportions. Statisticians tend to like things they can count, and some people are harder (more expensive) to count than others, particularly people in marginalized populations.\n\n### The $\\chi^2$ distribution\nThe $\\chi^2$ distribution is sometimes used to characterize data sets and statistics that\nare always positive and typically right skewed. Recall a normal distribution had two parameters –\nmean and standard deviation – that could be used to describe its exact characteristics. The $\\chi^2$\ndistribution has just one parameter called degrees of freedom (df), which influences the\nshape, center, and spread of the distribution. Here is a picture of the $\\chi^2$ distribution for several values of df (1--8).\n\n```{r}\n#| engine = 'tikz',\n#| engine.opts=list(extra.preamble=c(\"\\\\usepackage{pgfplots,stix,pagecolor}\",\"\\\\pgfplotsset{compat=1.18}\")),\n#| echo=FALSE\n\\begin{tikzpicture}[domain=.001:15,samples=200,thick]\n    \\clip (-1,-1) rectangle (15,10);\n    \\foreach[count=\\k,evaluate={\\z=\\k>2?\"(0,0)--\":\"\";\\c=10*\\k}]\n      \\g in {sqrt(pi),1,sqrt(pi)/2,1,3/4*sqrt(pi),2,15/8*sqrt(pi),6}\n        \\draw[color=blue!\\c!red,yscale=30] \\z\n          plot (\\x,{exp(ln(\\x/2)*\\k/2-ln(\\x)-\\x/2-ln(\\g))});\n  \\end{tikzpicture}\n```\n<!-- note that \\g are values of the gamma function for df=1:8 -->\n\nIn the jurors example, we can calculate the appropriate $p$-value in R by using the $\\chi^2$ statistic calculated from the sample, 5.89, and the parameter $k-1$ which is the number of groups minus one, using R:\n\n```{r}\npchisq(5.89,3,lower.tail=FALSE)\n```\n\nThis is a relatively large $p$-value given our earlier choices of cutoffs of 0.1, 0.05, and 0.01.\n\n### $\\chi^2$ test\n\nThe $\\chi^2$ test can be conducted in R for the juror example given in the book as follows.\n\n```{r}\no <- c(205,26,25,19)\ne <- c(198,19.25,33,24.75)/sum(o)\nchisq.test(o,p=e)\n```\n\nNote that I had to make an adjustment in R. The R variable `p` is supposed to be a vector of probabilities summing to 1. The way the table in the book presented it, it was not a vector of probabilities summing to one. So I divided each element of the input vector for `e` by the sum of the vector `o`.\n\n## Textbook section 6.4 Testing independence in 2-way tables\nSuppose you have a two way table. Datacamp gives an example of gender and sport as the two ways. The following data frame lists the number of males and females who like the following three sports: archery, boxing, and cycling. The $\\chi^2$ tests suggests that the genders are not independent for the three sports, meaning that the preferences may differ by gender.\n\n```{r}\nfemale <- c(35,15,50)\nmale <- c(10,30,60)\ndf <- cbind(male,female)\nrownames(df) <- c(\"archery\",\"boxing\",\"cycling\")\ndf\nchisq.test(df)\n```\n","srcMarkdownNoYaml":"\n\n```{r, include=FALSE}\nknitr::opts_chunk$set(\n  message=FALSE\n)\nlibrary(tidyverse)\n```\n\n## Recap Week 06\n\n- ggplot2\n- Foundations for Inference\n  - Point estimates\n  - Confidence Intervals\n  - Hypothesis Tests\n\n### A useful ggplot trick\n\nSome of you have struggled to produce boxplots of variables like `price` and `odometer` because they have ridiculous outliers. One way to overcome this that we've explored is to remove the extreme observations. A possibly better way, though, is illustrated below. Saying `outlier.shape=NA` in the `geom_boxplot()` function removes outliers before the dimensions of the plot are calculated. The result is a much more readable boxplot containing most of the cars.\n\n```{r}\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/vehicles.Rdata\"))\nlibrary(tidyverse)\ndf |> ggplot(aes(condition,odometer))+geom_boxplot(outlier.shape=NA)+  scale_y_continuous(limits = quantile(df$odometer, c(0.1, 0.8),na.rm=TRUE))\ndf |> ggplot(aes(condition,price))+geom_boxplot(outlier.shape=NA)+  scale_y_continuous(limits = quantile(df$price, c(0.1, 0.8),na.rm=TRUE))\n```\n\nNote that, in the above example, I've stored `df` in `vehicles.Rdata` using the `save()` function. Before saving it I converted `condition` to a factor and both `price` and `odometer` to numeric.\n\n### Preparing data\nNote that you can use the above quantile trick in other ways. For example, here I'm filtering out the extreme values of price, without necessarily knowing what they are.\n\n```{r}\ndf |>\n  filter(price >= quantile(price,.10) & price <= quantile(price,.90)) |>\n  count(state) |>\n  arrange(-n) |>\n  as_tibble() |>\n  print(n=12)\n```\n## Textbook section 6.1 Inference for a single proportion\n\n### Is the sample proportion nearly normal?\n\nNote: The book uses $p$ in two ways: as a $p$-value in a hypothesis test, and as $p$, a population proportion. Related to the population proportion is the sample proportion, $\\hat{p}$, pronounced p-hat. Too many $p$s!\n\nThe sampling distribution for $\\hat{p}$ based on a sample of size $n$ from a population with a true\nproportion $p$ is nearly normal when:\n\n1. The sample’s observations are independent, e.g., are from a simple random sample.\n2. We expected to see at least 10 successes and 10 failures in the sample, i.e., $np \\geqslant 10$ and $n(1 − p) \\geqslant 10$. This is called the success-failure condition.\n\nWhen these conditions are met, then the sampling distribution of $\\hat{p}$ is nearly normal with mean\n$p$ and standard error $\\text{SE}=\\sqrt{p(1-p)/n}$.\n\n### Confidence interval for a proportion\nA confidence interval provides a range of plausible values for the parameter $p$, and when $\\hat{p}$ can\nbe modeled using a normal distribution, the confidence interval for $p$ takes the form\n\n$$ \\hat{p} \\pm z^{*} \\times \\text{SE} $$\n\nwhere $z^{*}$ marks the $x$-axis for the selected confidence interval, e.g., 1.96 for a 95 percent confidence interval.\n\n### Prepare, Check, Calculate, Conclude Cycle\nThe OpenIntro Stats book recommends a four step cycle for both confidence intervals and hypothesis tests. It differs a bit from the seven step hypothesis testing method given in Week 06, but achieves the same result.\n\n- Prepare. Identify $\\hat{p}$ and $n$, and determine what confidence level you wish to use.\n- Check. Verify the conditions to ensure $\\hat{p}$ is nearly normal. For one-proportion confidence intervals, use $\\hat{p}$ in place of $p$ to check the success-failure condition.\n- Calculate. If the conditions hold, compute SE using $\\hat{p}$, find $z^{*}$, and construct the interval.\n- Conclude. Interpret the confidence interval in the context of the problem.\n\n### Same cycle for hypothesis testing for a proportion\n\n- Prepare. Identify the parameter of interest, list hypotheses, identify the significance level, and identify $\\hat{p}$ and $n$.\n- Check. Verify conditions to ensure $\\hat{p}$ is nearly normal under $H_0$. For one-proportion hypothesis tests, use the null value to check the success-failure condition.\n- Calculate. If the conditions hold, compute the standard error, again using $p_0$, compute the $Z$-score, and identify the $p$-value.\n- Conclude. Evaluate the hypothesis test by comparing the $p$-value to $\\alpha$, and provide a conclusion in the context of the problem.\n\n### Choosing sample size when estimating a proportion\nThis is probably the most important part of this section for practical purposes. The following expression denotes the *margin of error*:\n\n$$ z^{*}\\sqrt{\\frac{p(1-p)}{n}} $$\n\nYou have to choose the margin of error you want to report. The book gives an example of 0.04. So you want to find\n\n$$ z^{*}\\sqrt{\\frac{p(1-p)}{n}} < 0.04 $$\n\nThe problem is that you don't know $p$. Since the worst-case scenario is $p=0.5$, you have to use that unless you have some information about $p$. Recall that $z^{*}$ represents the $z$-score for the desired confidence level, so you have to choose that. The book gives an example where you want a 95 percent confidence level, so you choose 1.96. You could find this out in R by saying\n\n```{r}\nqnorm(0.025,lower.tail=FALSE)\n```\n\nreturning the $z$-score for the upper tail.\nThe reason for saying that 0.025 instead of 0.05 is that the probability of 0.05 is split between the tails.\nThe complementary function is `pnorm(1.959964,lower.tail=FALSE)`, which will return 0.025.\n\nOnce you have decided on values for $p$ and $z^*$, solve the above inequality for $n$.\n\n## Textbook section 6.2 Difference of two proportions\nIn this section, we're just modifying the previous section to account for a difference instead of a single proportion.\n\nThe difference $\\hat{p}_1-\\hat{p}_2$ can be modeled using the normal distribution when\n\n- The data are independent within and between the two groups (random samples or randomized experiment)\n- The success-failure condition holds for both groups (at least 10 successes and 10 failures in each sample)\n\n### Confidence intervals for $p_1-p_2$\n\n$$ \\text{point estimate } \\pm z^* \\times \\text{SE} \\rightarrow (\\hat{p}_1 - \\hat{p}_2) \\pm z^* \\times \\sqrt{\\frac{p_1(1-p_1)}{n_1}+\\frac{p_2(1-p_2)}{n_2}} $$\n\n### Hypothesis testing for $p_1-p_2$\n\nWhen the null hypothesis is that the proportions are equal, use the pooled proportion ($\\hat{p}_\\text{pooled}$)\nto verify the success-failure condition and estimate the standard error\n\n$$ \\hat{p}_\\text{pooled} = \\frac{\\text{number of “successes”}}{\\text{number of cases}} = \\frac{\\hat{p}_1n_1+\\hat{p}_2n_2}{n_1+n_2} $$\n\nHere $\\hat{p}_1n_1$ represents the number of successes in sample 1 since\n\n$$ \\hat{p}_1 = \\frac{\\text{number of successes in sample 1}}{n_1} $$\n\nSimilarly, $\\hat{p}_2n_2$ represents the number of successes in sample 2.\n\n## Textbook section 6.3 Testing goodness of fit using $\\chi^2$\n\nThe $\\chi^2$ test, pronounced k$\\overline{\\text{i}}$ square, is useful in many circumstances. The textbook treats two such circumstances:\n\n1. Suppose your sample can be divided into groups, as can the general population. Does your sample represent the general population?\n2. Does your sample resemble a particular distribution, such as the normal distribution?\n\nFor the first circumstance, we could divide a sample of people into races or genders and we would like to examine all at once for resemblance to the general population, rather than in pairs. The $\\chi^2$ statistic will permit an all-at-once comparison.\n\nThe $\\chi^2$ statistic is given by the following formula for $g$ groups.\n\n$$ \\chi^2 = \\frac{(\\text{observed count}_1-\\text{null count}_1)^2}{\\text{null count}_1} + \\cdots + \\frac{(\\text{observed count}_g-\\text{null count}_g)^2}{\\text{null count}_g} $$\n\nwhere the expression *null count* refers to the expected number of objects in the group. You have to be careful about how you determine the null count. For instance, the textbook gives an example of races of jurors. In such a case, the null counts should come from the population who can be selected as jurors. This might be a matter of some dispute since jurors are usually recruited through voting records and these records may not reflect the correct proportions. Statisticians tend to like things they can count, and some people are harder (more expensive) to count than others, particularly people in marginalized populations.\n\n### The $\\chi^2$ distribution\nThe $\\chi^2$ distribution is sometimes used to characterize data sets and statistics that\nare always positive and typically right skewed. Recall a normal distribution had two parameters –\nmean and standard deviation – that could be used to describe its exact characteristics. The $\\chi^2$\ndistribution has just one parameter called degrees of freedom (df), which influences the\nshape, center, and spread of the distribution. Here is a picture of the $\\chi^2$ distribution for several values of df (1--8).\n\n```{r}\n#| engine = 'tikz',\n#| engine.opts=list(extra.preamble=c(\"\\\\usepackage{pgfplots,stix,pagecolor}\",\"\\\\pgfplotsset{compat=1.18}\")),\n#| echo=FALSE\n\\begin{tikzpicture}[domain=.001:15,samples=200,thick]\n    \\clip (-1,-1) rectangle (15,10);\n    \\foreach[count=\\k,evaluate={\\z=\\k>2?\"(0,0)--\":\"\";\\c=10*\\k}]\n      \\g in {sqrt(pi),1,sqrt(pi)/2,1,3/4*sqrt(pi),2,15/8*sqrt(pi),6}\n        \\draw[color=blue!\\c!red,yscale=30] \\z\n          plot (\\x,{exp(ln(\\x/2)*\\k/2-ln(\\x)-\\x/2-ln(\\g))});\n  \\end{tikzpicture}\n```\n<!-- note that \\g are values of the gamma function for df=1:8 -->\n\nIn the jurors example, we can calculate the appropriate $p$-value in R by using the $\\chi^2$ statistic calculated from the sample, 5.89, and the parameter $k-1$ which is the number of groups minus one, using R:\n\n```{r}\npchisq(5.89,3,lower.tail=FALSE)\n```\n\nThis is a relatively large $p$-value given our earlier choices of cutoffs of 0.1, 0.05, and 0.01.\n\n### $\\chi^2$ test\n\nThe $\\chi^2$ test can be conducted in R for the juror example given in the book as follows.\n\n```{r}\no <- c(205,26,25,19)\ne <- c(198,19.25,33,24.75)/sum(o)\nchisq.test(o,p=e)\n```\n\nNote that I had to make an adjustment in R. The R variable `p` is supposed to be a vector of probabilities summing to 1. The way the table in the book presented it, it was not a vector of probabilities summing to one. So I divided each element of the input vector for `e` by the sum of the vector `o`.\n\n## Textbook section 6.4 Testing independence in 2-way tables\nSuppose you have a two way table. Datacamp gives an example of gender and sport as the two ways. The following data frame lists the number of males and females who like the following three sports: archery, boxing, and cycling. The $\\chi^2$ tests suggests that the genders are not independent for the three sports, meaning that the preferences may differ by gender.\n\n```{r}\nfemale <- c(35,15,50)\nmale <- c(10,30,60)\ndf <- cbind(male,female)\nrownames(df) <- c(\"archery\",\"boxing\",\"cycling\")\ndf\nchisq.test(df)\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"include-in-header":{"text":"<style>\n#quarto-sidebar .menu-text {\n  display: flex;\n}\n#quarto-sidebar .chapter-number {\n  display: block;\n  width: 1.5rem;\n  text-align: right;\n}\n#quarto-sidebar .chapter-title {\n  display: block;\n  padding-left: 8px;\n  text-indent: -2px;\n  width: 100%;\n}\n</style>\n"},"output-file":"week07.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","bibliography":["master.bib"],"mainfont":"Tex Gyre Schola","monofont":"JetBrainsMono Nerd Font","mathfont":"Tex Gyre Schola Math","theme":"cosmo","title":"More about Inference"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}