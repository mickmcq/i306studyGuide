{"title":"More Linear Regression; Logistic Regression","markdown":{"yaml":{"title":"More Linear Regression; Logistic Regression"},"headingText":"Recap week 9: Linear Regression","containsRefs":false,"markdown":"\n\n```{r, include=FALSE}\nknitr::opts_chunk$set(\n  message=FALSE\n)\nlibrary(tidyverse)\n```\n\n- Textbook section 8.2 Linear Regression\n- Textbook section 8.4 Inference for Linear Regression\n- Textbook section 9.1 Multiple Regression\n\n## More on Multiple regression\nThe OpenIntro Stats book gives an example of multiple regression with the `mariokart` data frame from their website. This involves the sale of 143 copies of the game *Mario Kart* for the Wii platform on eBay. They first predict the price based on most of the variables, like so.\n\n```{r}\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/mariokart.rda\"))\nm<-(lm(total_pr~cond+stock_photo+duration+wheels,data=mariokart))\nsummary(m)\nplot(m)\n```\n\nThere are four diagnostic plots in the above output. Each one gives us information about the quality of the model.\n\n### Residuals vs Fitted\nThis plot tells you the magnitude of the difference between the residuals and the fitted values. There are three things to watch for here. First, are there any drastic outliers? Yes, there are two, points 65 and 20. (Those are row numbers in the data frame.) You need to investigate those and decide whether to omit them from further analysis. Were they typos? Mismeasurements? Or is the process from which they derive intrinsically subject to occasional extreme variation. In the third case, you probably don't want to omit them.\n\nSecond, is the solid red line near the dashed zero line? Yes it is, indicating that the residuals have a mean of approximately zero. (The red line shows the mean of the residuals in the immediate region of the $x$-values of the observed data.)\n\nThird, is there a pattern to the residuals? No, there is not. The residuals appear to be of the same general magnitude at one end as the other. The things that would need action would be a curve or multiple curves, or a widening or narrowing shape, like the cross section of a horn.\n\n### Normal Q-Q\nThis is an important plot. I see many students erroneously claiming that residuals are normally distributed because they have a vague bell shape. That is not good enough to detect normality. The Q-Q plot is the standard way to detect normality. If the points lie along the dashed line, you can be reasonably safe in an assumption of normality. If they deviate from the dashed line, the residuals are probably not normally distributed.\n\n### Scale-Location\nLook for two things here. First, the red line should be approximately horizontal, meaning that there is not much variability in the standardized residuals. Second, look at the spread of the points around the red line. If they don't show a pattrn, this reinforces the assumption of homoscedasticity that we already found evidence for in the first plot.\n\n### Residuals vs Leverage\nThis shows you influential points that you may want to remove. Point 84 has high leverage (potential for influence) but is probably not actually very influential because it is so far from Cook's Distance. Points 20 and 65 are outliers but only point 20 is more than Cook's Distance away from the mean. In this case, you would likely remove point 20 from consideration unless there were a mitigating reason. For example, game collectors often pay extra for a game that has unusual attributes, such as shrink-wrapped original edition. As an example of a point you would definitely remove, draw a horizontal line from point 20 to a vertical line from point 84. Where they meet would be a high-leverage outlier that is unduly affecting the model no matter what it's underlying cause. On the other hand, what if you have many such points? Unfortunately, that probably means the model isn't very good.\n\n### Removing offending observations\nSuppose we want to get rid of points 20 and 65 and rerun the regression. We could either do this using plain R or the tidyverse. I prefer the tidyverse method because of clarity of exposition.\n\n```{r}\ndf <- mariokart |>\n  filter(!row_number() %in% c(20, 65))\nm<-(lm(total_pr~cond+stock_photo+duration+wheels,data=df))\nsummary(m)\nplot(m)\n```\n\nWhat a difference this makes in the output and the statistics and plots about the output!\nKeep in mind, though, that I just did this as an example. Points 20 and 65 may be totally legitimate in this case. Also, note that you could use plain R without the tidyverse to eliminate those rows by saying something like `df <- mariokart[-c(20,65),]`. The bracket notation assumes anything before the comma refers to a row and anything after a comma refers to a column. In this case, I didn't say anything about the columns, so the square brackets just have a dangling comma in them. The important point is that one method or another may seem more natural to you. For most students, the tidyverse approach is probably more natural, so I highlight that.\n\n## Logistic Regression\nLogistic regression is a kind classification rather than regression. The book doesn't make this point, but most textbooks do. You can divide machine learning problems into problems of regression and problems of classification. In regression, the $y$ variable is more or less continuous, whereas in the classification problem, $y$ is a set of categories, ordered or not. The word logistic comes from the logistic function, which is illustrated below. This interesting function takes an input from $-\\infty$ to $+\\infty$ and gives an output between zero and one. It can be used to reduce wildly varying inputs into a yes / no decision. It is also known as the sigmoid function.\n\n```{r}\n#| engine = 'tikz',\n#| engine.opts=list(extra.preamble=c(\"\\\\usepackage{pgfplots}\",\"\\\\pgfplotsset{compat=1.18}\")),\n#| out.width='50%',\n#| echo=FALSE\n\\begin{tikzpicture}\n\\begin{axis}%\n[\n    grid=major,\n    xmin=-6,\n    xmax=6,\n    axis x line=bottom,\n    ytick={0,.5,1},\n    ymax=1,\n    axis y line=middle,\n]\n    \\addplot%\n    [\n        blue,%\n        mark=none,\n        samples=100,\n        domain=-6:6,\n    ]\n    (x,{1/(1+exp(-x))});\n\\end{axis}\n\\end{tikzpicture}\n```\n\nNote that zero and one happen to be the boundaries of a probability measure. Hence, you can use the logistic function to reduce arbitrary numbers to a probability.\n\n```{r}\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/resume.rda\"))\nnames(resume)\nwith(resume,table(race,received_callback))\nwith(resume,table(gender,received_callback))\nwith(resume,table(honors,received_callback))\nsummary(glm(received_callback ~ honors,data=resume,family=\"binomial\"))\nsummary(glm(received_callback ~ race,data=resume,family=\"binomial\"))\nsummary(glm(received_callback ~ gender,data=resume,family=\"binomial\"))\n```\n\n### `tidymodels` approach\nDatacamp shows a different way, using `tidymodels` in one of their [tutorials](https://www.datacamp.com/tutorial/logistic-regression-R).\nIn this example, the bank wants to divide customers into those likely to buy and those unlikely to buy some banking product. They would like to divide the customers into these two groups using logistic regression, with a cutoff point of fifty-fifty. If there's better than a fifty-fifty chance, they will send a salesperson but if there's less than a fifty-fifty chance, they won't send a salesperson.\n\n```{r}\nlibrary(tidymodels)\n\n#. Read the dataset and convert the target variable to a factor\nbank_df <- read_csv2(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/bank-full.csv\"))\nbank_df$y = as.factor(bank_df$y)\n\n#. Plot job occupation against the target variable\nggplot(bank_df, aes(job, fill = y)) +\n    geom_bar() +\n    coord_flip()\n```\n\nA crucial concept you'll learn if you take a more advanced class, say 310D, is the notion of dividing data into two data frames, a training frame and a test frame. This is the conventional way to test machine learning models, of which logistic regression is one. You train the model on one set of data, then test it on another, previously unseen set. That's the next thing done in this example.\n\n```{r}\n#. Split data into train and test\nset.seed(421)\nsplit <- initial_split(bank_df, prop = 0.8, strata = y)\ntrain <- split |> \n         training()\ntest <- split |>\n        testing()\n#. Train a logistic regression model\nm <- logistic_reg(mixture = double(1), penalty = double(1)) |>\n  set_engine(\"glmnet\") |>\n  set_mode(\"classification\") |>\n  fit(y ~ ., data = train)\n\n#. Model summary\ntidy(m)\n#. Class Predictions\npred_class <- predict(m,\n                      new_data = test,\n                      type = \"class\")\n\n#. Class Probabilities\npred_proba <- predict(m,\n                      new_data = test,\n                      type = \"prob\")\nresults <- test |>\n           select(y) |>\n           bind_cols(pred_class, pred_proba)\n\naccuracy(results, truth = y, estimate = .pred_class)\n```\n\n### Hyperparameter tuning\nThere are aspects of this approach, called hyperparameters, that influence the quality of the model. It can be tedious to adjust these aspects, called penalty and mixture, so here's a technique for doing it automatically. You'll learn about this and similar techniques if you take a more advanced course like 310D, Intro to Data Science.\n\n```{r}\n#. Define the logistic regression model with penalty and mixture hyperparameters\nlog_reg <- logistic_reg(mixture = tune(), penalty = tune(), engine = \"glmnet\")\n\n#. Define the grid search for the hyperparameters\ngrid <- grid_regular(mixture(), penalty(), levels = c(mixture = 4, penalty = 3))\n\n#. Define the workflow for the model\nlog_reg_wf <- workflow() |>\n  add_model(log_reg) |>\n  add_formula(y ~ .)\n\n#. Define the resampling method for the grid search\nfolds <- vfold_cv(train, v = 5)\n\n#. Tune the hyperparameters using the grid search\nlog_reg_tuned <- tune_grid(\n  log_reg_wf,\n  resamples = folds,\n  grid = grid,\n  control = control_grid(save_pred = TRUE)\n)\n\nselect_best(log_reg_tuned, metric = \"roc_auc\")\n```\n\n```{r}\n#. Fit the model using the optimal hyperparameters\nlog_reg_final <- logistic_reg(penalty = 0.0000000001, mixture = 0) |>\n                 set_engine(\"glmnet\") |>\n                 set_mode(\"classification\") |>\n                 fit(y~., data = train)\n\n#. Evaluate the model performance on the testing set\npred_class <- predict(log_reg_final,\n                      new_data = test,\n                      type = \"class\")\nresults <- test |>\n  select(y) |>\n  bind_cols(pred_class, pred_proba)\n\n#. Create confusion matrix\nconf_mat(results, truth = y,\n         estimate = .pred_class)\nprecision(results, truth = y,\n          estimate = .pred_class)\nrecall(results, truth = y,\n          estimate = .pred_class)\n```\n\n### Evaluation metrics\nFollowing are two tables from @James2021 that you can use to evaluate a classification model.\n\n![](fiConfMatr.png)\n\n![](fiPrecisionRecall.png)\n\nAnother view is provided at [Wikipedia](https://en.wikipedia.org/wiki/Precision_and_recall) in the following picture\n\n![](fiPrecisionRecallPicture.png){fig-align=\"center\" width=50%}\n\n```{r}\ncoeff <- tidy(log_reg_final) |>\n  arrange(desc(abs(estimate))) |>\n  filter(abs(estimate) > 0.5)\ncoeff\nggplot(coeff, aes(x = term, y = estimate, fill = term)) +\n  geom_col() +\n  coord_flip()\n```\n","srcMarkdownNoYaml":"\n\n```{r, include=FALSE}\nknitr::opts_chunk$set(\n  message=FALSE\n)\nlibrary(tidyverse)\n```\n\n## Recap week 9: Linear Regression\n- Textbook section 8.2 Linear Regression\n- Textbook section 8.4 Inference for Linear Regression\n- Textbook section 9.1 Multiple Regression\n\n## More on Multiple regression\nThe OpenIntro Stats book gives an example of multiple regression with the `mariokart` data frame from their website. This involves the sale of 143 copies of the game *Mario Kart* for the Wii platform on eBay. They first predict the price based on most of the variables, like so.\n\n```{r}\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/mariokart.rda\"))\nm<-(lm(total_pr~cond+stock_photo+duration+wheels,data=mariokart))\nsummary(m)\nplot(m)\n```\n\nThere are four diagnostic plots in the above output. Each one gives us information about the quality of the model.\n\n### Residuals vs Fitted\nThis plot tells you the magnitude of the difference between the residuals and the fitted values. There are three things to watch for here. First, are there any drastic outliers? Yes, there are two, points 65 and 20. (Those are row numbers in the data frame.) You need to investigate those and decide whether to omit them from further analysis. Were they typos? Mismeasurements? Or is the process from which they derive intrinsically subject to occasional extreme variation. In the third case, you probably don't want to omit them.\n\nSecond, is the solid red line near the dashed zero line? Yes it is, indicating that the residuals have a mean of approximately zero. (The red line shows the mean of the residuals in the immediate region of the $x$-values of the observed data.)\n\nThird, is there a pattern to the residuals? No, there is not. The residuals appear to be of the same general magnitude at one end as the other. The things that would need action would be a curve or multiple curves, or a widening or narrowing shape, like the cross section of a horn.\n\n### Normal Q-Q\nThis is an important plot. I see many students erroneously claiming that residuals are normally distributed because they have a vague bell shape. That is not good enough to detect normality. The Q-Q plot is the standard way to detect normality. If the points lie along the dashed line, you can be reasonably safe in an assumption of normality. If they deviate from the dashed line, the residuals are probably not normally distributed.\n\n### Scale-Location\nLook for two things here. First, the red line should be approximately horizontal, meaning that there is not much variability in the standardized residuals. Second, look at the spread of the points around the red line. If they don't show a pattrn, this reinforces the assumption of homoscedasticity that we already found evidence for in the first plot.\n\n### Residuals vs Leverage\nThis shows you influential points that you may want to remove. Point 84 has high leverage (potential for influence) but is probably not actually very influential because it is so far from Cook's Distance. Points 20 and 65 are outliers but only point 20 is more than Cook's Distance away from the mean. In this case, you would likely remove point 20 from consideration unless there were a mitigating reason. For example, game collectors often pay extra for a game that has unusual attributes, such as shrink-wrapped original edition. As an example of a point you would definitely remove, draw a horizontal line from point 20 to a vertical line from point 84. Where they meet would be a high-leverage outlier that is unduly affecting the model no matter what it's underlying cause. On the other hand, what if you have many such points? Unfortunately, that probably means the model isn't very good.\n\n### Removing offending observations\nSuppose we want to get rid of points 20 and 65 and rerun the regression. We could either do this using plain R or the tidyverse. I prefer the tidyverse method because of clarity of exposition.\n\n```{r}\ndf <- mariokart |>\n  filter(!row_number() %in% c(20, 65))\nm<-(lm(total_pr~cond+stock_photo+duration+wheels,data=df))\nsummary(m)\nplot(m)\n```\n\nWhat a difference this makes in the output and the statistics and plots about the output!\nKeep in mind, though, that I just did this as an example. Points 20 and 65 may be totally legitimate in this case. Also, note that you could use plain R without the tidyverse to eliminate those rows by saying something like `df <- mariokart[-c(20,65),]`. The bracket notation assumes anything before the comma refers to a row and anything after a comma refers to a column. In this case, I didn't say anything about the columns, so the square brackets just have a dangling comma in them. The important point is that one method or another may seem more natural to you. For most students, the tidyverse approach is probably more natural, so I highlight that.\n\n## Logistic Regression\nLogistic regression is a kind classification rather than regression. The book doesn't make this point, but most textbooks do. You can divide machine learning problems into problems of regression and problems of classification. In regression, the $y$ variable is more or less continuous, whereas in the classification problem, $y$ is a set of categories, ordered or not. The word logistic comes from the logistic function, which is illustrated below. This interesting function takes an input from $-\\infty$ to $+\\infty$ and gives an output between zero and one. It can be used to reduce wildly varying inputs into a yes / no decision. It is also known as the sigmoid function.\n\n```{r}\n#| engine = 'tikz',\n#| engine.opts=list(extra.preamble=c(\"\\\\usepackage{pgfplots}\",\"\\\\pgfplotsset{compat=1.18}\")),\n#| out.width='50%',\n#| echo=FALSE\n\\begin{tikzpicture}\n\\begin{axis}%\n[\n    grid=major,\n    xmin=-6,\n    xmax=6,\n    axis x line=bottom,\n    ytick={0,.5,1},\n    ymax=1,\n    axis y line=middle,\n]\n    \\addplot%\n    [\n        blue,%\n        mark=none,\n        samples=100,\n        domain=-6:6,\n    ]\n    (x,{1/(1+exp(-x))});\n\\end{axis}\n\\end{tikzpicture}\n```\n\nNote that zero and one happen to be the boundaries of a probability measure. Hence, you can use the logistic function to reduce arbitrary numbers to a probability.\n\n```{r}\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/resume.rda\"))\nnames(resume)\nwith(resume,table(race,received_callback))\nwith(resume,table(gender,received_callback))\nwith(resume,table(honors,received_callback))\nsummary(glm(received_callback ~ honors,data=resume,family=\"binomial\"))\nsummary(glm(received_callback ~ race,data=resume,family=\"binomial\"))\nsummary(glm(received_callback ~ gender,data=resume,family=\"binomial\"))\n```\n\n### `tidymodels` approach\nDatacamp shows a different way, using `tidymodels` in one of their [tutorials](https://www.datacamp.com/tutorial/logistic-regression-R).\nIn this example, the bank wants to divide customers into those likely to buy and those unlikely to buy some banking product. They would like to divide the customers into these two groups using logistic regression, with a cutoff point of fifty-fifty. If there's better than a fifty-fifty chance, they will send a salesperson but if there's less than a fifty-fifty chance, they won't send a salesperson.\n\n```{r}\nlibrary(tidymodels)\n\n#. Read the dataset and convert the target variable to a factor\nbank_df <- read_csv2(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/bank-full.csv\"))\nbank_df$y = as.factor(bank_df$y)\n\n#. Plot job occupation against the target variable\nggplot(bank_df, aes(job, fill = y)) +\n    geom_bar() +\n    coord_flip()\n```\n\nA crucial concept you'll learn if you take a more advanced class, say 310D, is the notion of dividing data into two data frames, a training frame and a test frame. This is the conventional way to test machine learning models, of which logistic regression is one. You train the model on one set of data, then test it on another, previously unseen set. That's the next thing done in this example.\n\n```{r}\n#. Split data into train and test\nset.seed(421)\nsplit <- initial_split(bank_df, prop = 0.8, strata = y)\ntrain <- split |> \n         training()\ntest <- split |>\n        testing()\n#. Train a logistic regression model\nm <- logistic_reg(mixture = double(1), penalty = double(1)) |>\n  set_engine(\"glmnet\") |>\n  set_mode(\"classification\") |>\n  fit(y ~ ., data = train)\n\n#. Model summary\ntidy(m)\n#. Class Predictions\npred_class <- predict(m,\n                      new_data = test,\n                      type = \"class\")\n\n#. Class Probabilities\npred_proba <- predict(m,\n                      new_data = test,\n                      type = \"prob\")\nresults <- test |>\n           select(y) |>\n           bind_cols(pred_class, pred_proba)\n\naccuracy(results, truth = y, estimate = .pred_class)\n```\n\n### Hyperparameter tuning\nThere are aspects of this approach, called hyperparameters, that influence the quality of the model. It can be tedious to adjust these aspects, called penalty and mixture, so here's a technique for doing it automatically. You'll learn about this and similar techniques if you take a more advanced course like 310D, Intro to Data Science.\n\n```{r}\n#. Define the logistic regression model with penalty and mixture hyperparameters\nlog_reg <- logistic_reg(mixture = tune(), penalty = tune(), engine = \"glmnet\")\n\n#. Define the grid search for the hyperparameters\ngrid <- grid_regular(mixture(), penalty(), levels = c(mixture = 4, penalty = 3))\n\n#. Define the workflow for the model\nlog_reg_wf <- workflow() |>\n  add_model(log_reg) |>\n  add_formula(y ~ .)\n\n#. Define the resampling method for the grid search\nfolds <- vfold_cv(train, v = 5)\n\n#. Tune the hyperparameters using the grid search\nlog_reg_tuned <- tune_grid(\n  log_reg_wf,\n  resamples = folds,\n  grid = grid,\n  control = control_grid(save_pred = TRUE)\n)\n\nselect_best(log_reg_tuned, metric = \"roc_auc\")\n```\n\n```{r}\n#. Fit the model using the optimal hyperparameters\nlog_reg_final <- logistic_reg(penalty = 0.0000000001, mixture = 0) |>\n                 set_engine(\"glmnet\") |>\n                 set_mode(\"classification\") |>\n                 fit(y~., data = train)\n\n#. Evaluate the model performance on the testing set\npred_class <- predict(log_reg_final,\n                      new_data = test,\n                      type = \"class\")\nresults <- test |>\n  select(y) |>\n  bind_cols(pred_class, pred_proba)\n\n#. Create confusion matrix\nconf_mat(results, truth = y,\n         estimate = .pred_class)\nprecision(results, truth = y,\n          estimate = .pred_class)\nrecall(results, truth = y,\n          estimate = .pred_class)\n```\n\n### Evaluation metrics\nFollowing are two tables from @James2021 that you can use to evaluate a classification model.\n\n![](fiConfMatr.png)\n\n![](fiPrecisionRecall.png)\n\nAnother view is provided at [Wikipedia](https://en.wikipedia.org/wiki/Precision_and_recall) in the following picture\n\n![](fiPrecisionRecallPicture.png){fig-align=\"center\" width=50%}\n\n```{r}\ncoeff <- tidy(log_reg_final) |>\n  arrange(desc(abs(estimate))) |>\n  filter(abs(estimate) > 0.5)\ncoeff\nggplot(coeff, aes(x = term, y = estimate, fill = term)) +\n  geom_col() +\n  coord_flip()\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"include-in-header":{"text":"<style>\n#quarto-sidebar .menu-text {\n  display: flex;\n}\n#quarto-sidebar .chapter-number {\n  display: block;\n  width: 1.5rem;\n  text-align: right;\n}\n#quarto-sidebar .chapter-title {\n  display: block;\n  padding-left: 8px;\n  text-indent: -2px;\n  width: 100%;\n}\n</style>\n"},"output-file":"week10.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","bibliography":["master.bib"],"mainfont":"Tex Gyre Schola","monofont":"JetBrainsMono Nerd Font","mathfont":"Tex Gyre Schola Math","theme":"cosmo","title":"More Linear Regression; Logistic Regression"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}