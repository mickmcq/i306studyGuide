{"title":"More about R and Quarto","markdown":{"yaml":{"title":"More about R and Quarto"},"headingText":"Recap Week 02","containsRefs":false,"markdown":"\n\n```{r, include=FALSE}\nknitr::opts_chunk$set(\n  message=FALSE\n)\n```\n\n\nWe did some exercises, for which there are now solutions in the file `week02exercises-soln.qmd` and `week02exercises-soln.html`. You should examine and compare these two files, especially the exercise parts.\n\n## Week 03: More on R and Quarto\n\nWe will establish groups for the milestones. I'm open to moving you around if needed, subject to the constraint that we have no more than five members in a group. I expect to have four groups of four and two groups of five.\n\n### The template files\n\n### Categorical Data\n\nThe template files show how you can begin the milestones. Notice that I have named the data frame as `df` in the `template.qmd` file. As a result, I can write functions like the following.\n\n```{r}\nlibrary(tidyverse)\ndf <- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/vehicles.csv\"))\ndf |> count(title_status,sort=TRUE)\n```\n\nThe above is a good way to investigate categorical data. Notice that I've used the pipe character, so that the data frame `df` is sent to the `count()` function. Then a particular categorical column of `df` is counted and sorted in descending numerical order. The `count()` function is part of the `dplyr` package, which is one of the `tidyverse` packages. You only need to load the `tidyverse` set of packages once in a document, preferably near the beginning.\n\nAnother point about the above result is that the `count()` function behaves different for large numbers of values. For example, `region` has 404 values. The `count()` function will just display the first and last 5 by default. I can make it work by adding the print fuction:\n\n```{r}\ndf |> count(region,sort=TRUE) |> print(n=404)\n```\n\nThe number 404 is because there are 404 unique elements in the\nlist, which you can find out by looking at the output of the `count()` function.\n\nAnother solution (more cumbersome) follows.\n\n```{r}\ndf %>%\n  group_by(region) %>%\n  do(data.frame(nrow=nrow(.))) %>%\n  arrange(desc(nrow)) %>%\n  print(n=40)\n```\n\nAnother way to investigate categorical data is through contingency tables. You have already made some of these using the `table()` function and some associated functions that are mentioned in the `week02exercises-soln.qmd` file. It will take some intuition to figure out which pairs of categorical columns should be tabulated for Milestone 1.\n\n### Numerical Data\n\nFor numerical data, you can do what we did last week to investigate a single column.\n\n```{r}\nsummary(df$price)\n```\n\nor\n\n```{r}\nlibrary(scales)\ndf$price <- as.double(df$price)\ndf %>%\n    summarize(Min=comma(min(price)),\n              firstq=comma(quantile(price,0.25)),\n              Median=comma(median(price)),\n              Mean=comma(mean(price)),\n              thirdq=comma(quantile(price,0.75)),\n              Max=comma(max(price)))\ndf$price <- as.integer(df$price)\n```\n\nYou'll notice that one of the prices is 3.7 billion dollars for a Toyota truck! This is obviously a misprint! You should probably remove this row from the data frame and save the data frame without it. A more sophisticated alternative would be to impute a value for this truck. There are many advanced statistical ways to do this, but they are beyond the scope of this course. @vanBuuren2018 describes several excellent ways to do so, particularly in that book's Section 5.1.1, Recommended Workflows. It is usually a mistake to use an average for missing (NA) values because to do so compresses the variance unnaturally. You may remove that particular row by saying `df <- df[-(which.max(df$price)),]`. Unfortunately, you will find that not to be very useful because there are several prices of over a billion dollars for generic cars! In fact, there are several with prices listed as `1234567890` and some listed at `987654321`. Many other ridiculous patterns can be found for price. So what can you do? Personally, I might add the following line right after reading in the file:\n\n```{r}\n#| eval: FALSE\ndf <- df[df$price<100000&df$price>0,]\n```\n\nor, better yet,\n\n```{r}\ndf <- df[df$price < quantile(df$price,.9,na.rm=TRUE) & df$price > quantile(df$price,.1,na.rm=TRUE),]\n```\n\nThe first one will rid the data frame of cars priced at greater than 100,000 dollars and still leave you with over 300,000 automobiles to analyze. Of course, there are will still be spurious entries, but at least it's a start. The first code will also get rid of cars priced at exactly zero dollars. Examination of the data frame will show that many of the zero dollar entries are just ads for used car dealers. The expression `price<100000&price>0` is called a compound Boolean expression. It is compound because of the ampersand, which stands for the word *and*. It means that the row has to contain a price less than 100,000 AND it also has to be a price greater than zero.\n\nThe second code gets rid of the 90th percentile and above and the 10th percentile and below, which will still leave plenty.\n\nBy the way, this is a good reminder that a data frame has a row and a column index. You can refer to rows by saying `df[`*expression*`,]` and to columns by saying `df[,`*expression*`]`. The *expression* can be any mathematical expression that resolves to TRUE or FALSE. The first above expression resolves to TRUE for cars priced at greater than zero but less than 100,000 dollars, and FALSE for cars priced at any integer greater than or equal to 100,000. You can tell that `price` is a 64 bit integer by saying `str(df)` which will tell the structure of the `df` data frame. You should be able to see that most of the columns are classified as `chr` or character. This is not desirable. Most of the columns clasified as `chr` should more properly be classified as factors. Factors take up less space on your computer, are faster to process, and allow more types of processing than `chr`. Unfortunately, if you store your intermediate work as a `.csv` file, you will lose the factor designation. Therefore, I recommend that you do the following.\n\nStep 1. Get rid of rows you don't want, such as those with prices over or under some threshold value you choose.\n\nStep 2. Get rid of columns you don't want to analyze, such as url or VIN.\n\nStep 3. Convert some of the `chr` columns to factor. For instance, you can say `df$state <- as.factor(df$state)`.\n\nStep 4. Save your file by saying something like `save(df,file=\"df.RData\")`\n\nStep 5. Quit using this file and open a file called `intermediate.qmd`\n\nStep 6. At the beginning of that file, say `load(\"df.RData\")`.\n\nStep 7. Do all your work in that file, then paste the work back into your `template.qmd` file so you can run it as required. (Remember, you are not turning in a `.RData` file. Your `m1.qmd` file must start with reading in the `vehicles.csv` file and do processing on the resulting data frame.)\n\nStep 8. Merge your `template.qmd` file with those of your group members into one `m1.qmd` file. For example, you could name all your individual template files with your names and one group member could merge them together. This should be easy for Milestone 1 since an obvious way to divide up your work is to assign different columns to different group members.\n\n### Combining numerical and categorical selection\n\n```{r}\ndfX <- subset(df,state %in% c(\"ca\",\"ny\") & type %in% c(\"sedan\",\"SUV\") & price<99999 & price>0)\ntbl <- table(dfX$state,dfX$type)\naddmargins(tbl)\n```\n\nAbove is an example of getting a small contingency table with only the data you want. The first line selects only cars offered in ca or ny, only sedans or SUVs, and only with prices below 99,999 dollars and more than zero dollars.\nThen we can make a compact contingency table of that new data frame and add the margins to it.\n\n### Getting the data displayed as you wish\nSomeone asked me how to display price ranges by\nmanufacturer. Here's one way to do that:\n\n```{r}\ndf |>\n  group_by(manufacturer) |>\n  reframe(min = min(price),max=max(price)) |>\n  print(n=43)\n```\n\nThe number 43 is because there are 43 manufacturers in\nthe data frame. Note that, using the `reframe()`\nfunction, I could add a few more comma-separated statistics to the\noutput.\n\n### Investigating words\n\nTo make a word cloud, I first exported the `model` column from the data frame to a file called `bla`, using the `write_csv()` function. Next I used Vim to convert all spaces to newlines, so that the file would have one word on each line. To do so I said `:%s/ /\\r/g` in Vim.\n\nNext I said `sort bla | uniq -c | sort -n >blabla` to get the following output. This is just the last few lines of the file. Note that the most frequently occuring word in the file is 1500, which occurs 24,014 times.\n\n```\n3383    fusion\n3475    tundra\n3479    xl\n3528    corolla\n3585    unlimited\n3985    altima\n3985    explorer\n4072    3500\n4105    f-250\n4256    mustang\n4277    escape\n5162    camry\n5208    series\n5244    pickup\n5277    NA\n5479    accord\n5585    xlt\n5659    awd\n5660    cherokee\n5667    f150\n5680    tacoma\n5734    civic\n5744    s\n5865    2d\n6185    limited\n6213    lt\n6295    coupe\n6519    crew\n6869    premium\n7190    duty\n7319    se\n7531    2500\n7564    utility\n8093    wrangler\n8327    super\n8642    sierra\n8733    grand\n9578    4x4\n10283   f-150\n14877   sedan\n15152   cab\n17181   silverado\n17488   4d\n23130   sport\n24014   1500\n```\n\nNext I opened the `blabla` file in Vim and converted all sequences of spaces to a tab character, saying `:%s/^  *//` to get rid of leading spaces, then `:%s/  */\\t/` to convert the intercolumn spaces to tabs. I saved this file as `wordfreq.tsv` and opened it in R, using the following code to convert it to a word cloud.\n\n```{r}\nlibrary(tidyverse)\ndf<-read_tsv(\"wordfreq.tsv\")\ndf<-df|>relocate(freq,.after=word)\nhead(df)\nlibrary(wordcloud2)\nwordcloud2(df)\n```\n\nI was not able to reproduce the error message I kept getting in class. This simply worked the first time through after class. I also can not explain why several of the most frequently occurring words do not appear in the word cloud. I suspect it is because the word cloud is truncated in this display. When I have constructed this word cloud previously, it was ellipse-shaped and zoomed out. I have forgotten whatever I did to make that happen!\n\n### Filtering two specific trucks\n\nTwo of the most common words I found above were *f-150* and *silverado*. Since these are two popular truck models, I thought to compare them by making a data frame for each. To do so I used the `filter()` function of the `dplyr` package. This is described in great detail in Chapter 4 of @Wickham2023. The special construction `(?i)` makes whatever follows case-insensitive. Thus, in this case, it picks up Silverado and SILVERADO, as well as silverado. It would also pick up sIlVeRaDo if such a strange mixed case version presented itself. This is called a regular expression or regex and is commonly used in finding and replacing text patterns.\n\nThe regexes for f-150 are much more complicated. First, I used the alternating selector `[Ff]`. This stands for either a capital F or small f but not both. I can put any number of characters in the brackets and this will select any of them occurring one time. Next, I used the *zero-or one character selector*, which consists of a dot followed by a question mark. That selector attaches itself to whatever precedes it, in this case `[Ff]`. So the whole construct `[Ff].?` can be read as \"exactly one F or f followed by exactly one character.\" Next comes a literal 150, so the only time that an F or f plus at most one character will be matched is if it is immediately followed by `150`. The last construct in this regular expression is the negating alternator, `[^]` with a zero in it. The negating alternator matches anything *except* the characters following the `^` in brackets. In this case it means that any character except a zero is okay. It's actually a vestige of an earlier attempt. I had previously written the expression as `[Ff].*150` and run it and it erroneously picked up a model string that said \"pacifica \\$1500\". That was because I used a `*` instead of a `?`. The `*` symbol means *zero or more characters*. As a result, the words \"pacifica \\$1500\" matched because there is an f followed by some characters, followed by 150! So I stuck the negating alternator `[^0]` in to get rid of the 15000 before I realized that the real problem was the `*`. I report this so you can see that it is a potentially long iterative process to find the right regex. This regex picks up f150, F 150, f-150, and so on. I could have refined it further. Can you see how?\n\n```{r}\n#| label: filteringTrucks\nlibrary(tidyverse)\n#df <- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/vehicles.csv\"))\n#load(\"/home/mm223266/data/vehicles.Rdata\")\n#df <- read_csv(\"/home/mm223266/data/vehicles.csv\")\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/vehicles.Rdata\"))\n#names(df)\ndf2 <- df |> filter(str_detect(df$model,regex(\"(?i)silverado\")))\nhead(table(df2$model),12)\nprint(\"::::::::::::::::::::::::::::::::::\")\ndf3 <- df |> filter(str_detect(df$model,regex(\"[Ff].?150[^0]\")))\nhead(table(df3$model),12)\n```\n\n### A function between numeric and visual\n\nMilestone 1 is supposed to be entirely about numeric descriptions of the data, not visual descriptions, which will be covered in Milestone 2. Yet there is one function that exists in a gray area between numeric and graphical. That is the stem and leaf plot. Consider the following output.\n\n```{r}\nstem(df$odometer)\n```\n\nThe output does not need any graphical processor. It is only characters that can be included in text. Yet it is a kind of graphic because you can see, for instance, that of the cars have either very little mileage on the odometer or very much. Read it like this:\n\n- The *stem* is the vertical line.\n- The numbers to the left of the stem are, in this case, numbers in the sixth place to the left of the decimal point. In other words the first row represents zero to 999999.\n- Each character to the right of the stem represents one car. There are probably 80 zeros in the first row. The `+390151` indicates that there are 390,151 cars in that category that are not represented. The numbers in these cases represent the next significant digit after the one on the stem.\n- It is probably easier to read a stem and leaf plot for a smaller data frame, in the following case for the first 100 cars in the above data frame.\n\n```{r}\nstem(df$odometer[1:100])\n```\n\nThese entries come from a reduced data frame where I first ran the above code, getting rid of the high-priced and free cars. It may make it easier to understand to look at the entries themselves.\n\n```{r}\nhead(df$odometer,n=100L) |> sort(decreasing=TRUE)\n```\n\nThe very first row in the stem and leaf plot above counts cars priced at less than 20,000 dollars. There are 28 of them. They all have 0 or 1 in the fifth position to the left of the decimal. Only one of those, which is offered at 21 dollars, has zeros in both of the first two positions. It is the very first entry after the stem, represented as a zero. The next three entries are the cars that sell for the next lowest prices, between zero and 2 in the next decimal position. They are represented as 2s. You can see at a glance that, in this group of 100 cars, the lower odometer readings predominate. By the way, the `stem()` function discards `NA` values before processing the remainder. So there are only a total of 78 characters to the right of the stem on all the rows put together.\n\nThere is some difference of opinion as to whether `stem()` is graphical or numerical. What do you think?\n\n","srcMarkdownNoYaml":"\n\n```{r, include=FALSE}\nknitr::opts_chunk$set(\n  message=FALSE\n)\n```\n\n## Recap Week 02\n\nWe did some exercises, for which there are now solutions in the file `week02exercises-soln.qmd` and `week02exercises-soln.html`. You should examine and compare these two files, especially the exercise parts.\n\n## Week 03: More on R and Quarto\n\nWe will establish groups for the milestones. I'm open to moving you around if needed, subject to the constraint that we have no more than five members in a group. I expect to have four groups of four and two groups of five.\n\n### The template files\n\n### Categorical Data\n\nThe template files show how you can begin the milestones. Notice that I have named the data frame as `df` in the `template.qmd` file. As a result, I can write functions like the following.\n\n```{r}\nlibrary(tidyverse)\ndf <- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/vehicles.csv\"))\ndf |> count(title_status,sort=TRUE)\n```\n\nThe above is a good way to investigate categorical data. Notice that I've used the pipe character, so that the data frame `df` is sent to the `count()` function. Then a particular categorical column of `df` is counted and sorted in descending numerical order. The `count()` function is part of the `dplyr` package, which is one of the `tidyverse` packages. You only need to load the `tidyverse` set of packages once in a document, preferably near the beginning.\n\nAnother point about the above result is that the `count()` function behaves different for large numbers of values. For example, `region` has 404 values. The `count()` function will just display the first and last 5 by default. I can make it work by adding the print fuction:\n\n```{r}\ndf |> count(region,sort=TRUE) |> print(n=404)\n```\n\nThe number 404 is because there are 404 unique elements in the\nlist, which you can find out by looking at the output of the `count()` function.\n\nAnother solution (more cumbersome) follows.\n\n```{r}\ndf %>%\n  group_by(region) %>%\n  do(data.frame(nrow=nrow(.))) %>%\n  arrange(desc(nrow)) %>%\n  print(n=40)\n```\n\nAnother way to investigate categorical data is through contingency tables. You have already made some of these using the `table()` function and some associated functions that are mentioned in the `week02exercises-soln.qmd` file. It will take some intuition to figure out which pairs of categorical columns should be tabulated for Milestone 1.\n\n### Numerical Data\n\nFor numerical data, you can do what we did last week to investigate a single column.\n\n```{r}\nsummary(df$price)\n```\n\nor\n\n```{r}\nlibrary(scales)\ndf$price <- as.double(df$price)\ndf %>%\n    summarize(Min=comma(min(price)),\n              firstq=comma(quantile(price,0.25)),\n              Median=comma(median(price)),\n              Mean=comma(mean(price)),\n              thirdq=comma(quantile(price,0.75)),\n              Max=comma(max(price)))\ndf$price <- as.integer(df$price)\n```\n\nYou'll notice that one of the prices is 3.7 billion dollars for a Toyota truck! This is obviously a misprint! You should probably remove this row from the data frame and save the data frame without it. A more sophisticated alternative would be to impute a value for this truck. There are many advanced statistical ways to do this, but they are beyond the scope of this course. @vanBuuren2018 describes several excellent ways to do so, particularly in that book's Section 5.1.1, Recommended Workflows. It is usually a mistake to use an average for missing (NA) values because to do so compresses the variance unnaturally. You may remove that particular row by saying `df <- df[-(which.max(df$price)),]`. Unfortunately, you will find that not to be very useful because there are several prices of over a billion dollars for generic cars! In fact, there are several with prices listed as `1234567890` and some listed at `987654321`. Many other ridiculous patterns can be found for price. So what can you do? Personally, I might add the following line right after reading in the file:\n\n```{r}\n#| eval: FALSE\ndf <- df[df$price<100000&df$price>0,]\n```\n\nor, better yet,\n\n```{r}\ndf <- df[df$price < quantile(df$price,.9,na.rm=TRUE) & df$price > quantile(df$price,.1,na.rm=TRUE),]\n```\n\nThe first one will rid the data frame of cars priced at greater than 100,000 dollars and still leave you with over 300,000 automobiles to analyze. Of course, there are will still be spurious entries, but at least it's a start. The first code will also get rid of cars priced at exactly zero dollars. Examination of the data frame will show that many of the zero dollar entries are just ads for used car dealers. The expression `price<100000&price>0` is called a compound Boolean expression. It is compound because of the ampersand, which stands for the word *and*. It means that the row has to contain a price less than 100,000 AND it also has to be a price greater than zero.\n\nThe second code gets rid of the 90th percentile and above and the 10th percentile and below, which will still leave plenty.\n\nBy the way, this is a good reminder that a data frame has a row and a column index. You can refer to rows by saying `df[`*expression*`,]` and to columns by saying `df[,`*expression*`]`. The *expression* can be any mathematical expression that resolves to TRUE or FALSE. The first above expression resolves to TRUE for cars priced at greater than zero but less than 100,000 dollars, and FALSE for cars priced at any integer greater than or equal to 100,000. You can tell that `price` is a 64 bit integer by saying `str(df)` which will tell the structure of the `df` data frame. You should be able to see that most of the columns are classified as `chr` or character. This is not desirable. Most of the columns clasified as `chr` should more properly be classified as factors. Factors take up less space on your computer, are faster to process, and allow more types of processing than `chr`. Unfortunately, if you store your intermediate work as a `.csv` file, you will lose the factor designation. Therefore, I recommend that you do the following.\n\nStep 1. Get rid of rows you don't want, such as those with prices over or under some threshold value you choose.\n\nStep 2. Get rid of columns you don't want to analyze, such as url or VIN.\n\nStep 3. Convert some of the `chr` columns to factor. For instance, you can say `df$state <- as.factor(df$state)`.\n\nStep 4. Save your file by saying something like `save(df,file=\"df.RData\")`\n\nStep 5. Quit using this file and open a file called `intermediate.qmd`\n\nStep 6. At the beginning of that file, say `load(\"df.RData\")`.\n\nStep 7. Do all your work in that file, then paste the work back into your `template.qmd` file so you can run it as required. (Remember, you are not turning in a `.RData` file. Your `m1.qmd` file must start with reading in the `vehicles.csv` file and do processing on the resulting data frame.)\n\nStep 8. Merge your `template.qmd` file with those of your group members into one `m1.qmd` file. For example, you could name all your individual template files with your names and one group member could merge them together. This should be easy for Milestone 1 since an obvious way to divide up your work is to assign different columns to different group members.\n\n### Combining numerical and categorical selection\n\n```{r}\ndfX <- subset(df,state %in% c(\"ca\",\"ny\") & type %in% c(\"sedan\",\"SUV\") & price<99999 & price>0)\ntbl <- table(dfX$state,dfX$type)\naddmargins(tbl)\n```\n\nAbove is an example of getting a small contingency table with only the data you want. The first line selects only cars offered in ca or ny, only sedans or SUVs, and only with prices below 99,999 dollars and more than zero dollars.\nThen we can make a compact contingency table of that new data frame and add the margins to it.\n\n### Getting the data displayed as you wish\nSomeone asked me how to display price ranges by\nmanufacturer. Here's one way to do that:\n\n```{r}\ndf |>\n  group_by(manufacturer) |>\n  reframe(min = min(price),max=max(price)) |>\n  print(n=43)\n```\n\nThe number 43 is because there are 43 manufacturers in\nthe data frame. Note that, using the `reframe()`\nfunction, I could add a few more comma-separated statistics to the\noutput.\n\n### Investigating words\n\nTo make a word cloud, I first exported the `model` column from the data frame to a file called `bla`, using the `write_csv()` function. Next I used Vim to convert all spaces to newlines, so that the file would have one word on each line. To do so I said `:%s/ /\\r/g` in Vim.\n\nNext I said `sort bla | uniq -c | sort -n >blabla` to get the following output. This is just the last few lines of the file. Note that the most frequently occuring word in the file is 1500, which occurs 24,014 times.\n\n```\n3383    fusion\n3475    tundra\n3479    xl\n3528    corolla\n3585    unlimited\n3985    altima\n3985    explorer\n4072    3500\n4105    f-250\n4256    mustang\n4277    escape\n5162    camry\n5208    series\n5244    pickup\n5277    NA\n5479    accord\n5585    xlt\n5659    awd\n5660    cherokee\n5667    f150\n5680    tacoma\n5734    civic\n5744    s\n5865    2d\n6185    limited\n6213    lt\n6295    coupe\n6519    crew\n6869    premium\n7190    duty\n7319    se\n7531    2500\n7564    utility\n8093    wrangler\n8327    super\n8642    sierra\n8733    grand\n9578    4x4\n10283   f-150\n14877   sedan\n15152   cab\n17181   silverado\n17488   4d\n23130   sport\n24014   1500\n```\n\nNext I opened the `blabla` file in Vim and converted all sequences of spaces to a tab character, saying `:%s/^  *//` to get rid of leading spaces, then `:%s/  */\\t/` to convert the intercolumn spaces to tabs. I saved this file as `wordfreq.tsv` and opened it in R, using the following code to convert it to a word cloud.\n\n```{r}\nlibrary(tidyverse)\ndf<-read_tsv(\"wordfreq.tsv\")\ndf<-df|>relocate(freq,.after=word)\nhead(df)\nlibrary(wordcloud2)\nwordcloud2(df)\n```\n\nI was not able to reproduce the error message I kept getting in class. This simply worked the first time through after class. I also can not explain why several of the most frequently occurring words do not appear in the word cloud. I suspect it is because the word cloud is truncated in this display. When I have constructed this word cloud previously, it was ellipse-shaped and zoomed out. I have forgotten whatever I did to make that happen!\n\n### Filtering two specific trucks\n\nTwo of the most common words I found above were *f-150* and *silverado*. Since these are two popular truck models, I thought to compare them by making a data frame for each. To do so I used the `filter()` function of the `dplyr` package. This is described in great detail in Chapter 4 of @Wickham2023. The special construction `(?i)` makes whatever follows case-insensitive. Thus, in this case, it picks up Silverado and SILVERADO, as well as silverado. It would also pick up sIlVeRaDo if such a strange mixed case version presented itself. This is called a regular expression or regex and is commonly used in finding and replacing text patterns.\n\nThe regexes for f-150 are much more complicated. First, I used the alternating selector `[Ff]`. This stands for either a capital F or small f but not both. I can put any number of characters in the brackets and this will select any of them occurring one time. Next, I used the *zero-or one character selector*, which consists of a dot followed by a question mark. That selector attaches itself to whatever precedes it, in this case `[Ff]`. So the whole construct `[Ff].?` can be read as \"exactly one F or f followed by exactly one character.\" Next comes a literal 150, so the only time that an F or f plus at most one character will be matched is if it is immediately followed by `150`. The last construct in this regular expression is the negating alternator, `[^]` with a zero in it. The negating alternator matches anything *except* the characters following the `^` in brackets. In this case it means that any character except a zero is okay. It's actually a vestige of an earlier attempt. I had previously written the expression as `[Ff].*150` and run it and it erroneously picked up a model string that said \"pacifica \\$1500\". That was because I used a `*` instead of a `?`. The `*` symbol means *zero or more characters*. As a result, the words \"pacifica \\$1500\" matched because there is an f followed by some characters, followed by 150! So I stuck the negating alternator `[^0]` in to get rid of the 15000 before I realized that the real problem was the `*`. I report this so you can see that it is a potentially long iterative process to find the right regex. This regex picks up f150, F 150, f-150, and so on. I could have refined it further. Can you see how?\n\n```{r}\n#| label: filteringTrucks\nlibrary(tidyverse)\n#df <- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/vehicles.csv\"))\n#load(\"/home/mm223266/data/vehicles.Rdata\")\n#df <- read_csv(\"/home/mm223266/data/vehicles.csv\")\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/vehicles.Rdata\"))\n#names(df)\ndf2 <- df |> filter(str_detect(df$model,regex(\"(?i)silverado\")))\nhead(table(df2$model),12)\nprint(\"::::::::::::::::::::::::::::::::::\")\ndf3 <- df |> filter(str_detect(df$model,regex(\"[Ff].?150[^0]\")))\nhead(table(df3$model),12)\n```\n\n### A function between numeric and visual\n\nMilestone 1 is supposed to be entirely about numeric descriptions of the data, not visual descriptions, which will be covered in Milestone 2. Yet there is one function that exists in a gray area between numeric and graphical. That is the stem and leaf plot. Consider the following output.\n\n```{r}\nstem(df$odometer)\n```\n\nThe output does not need any graphical processor. It is only characters that can be included in text. Yet it is a kind of graphic because you can see, for instance, that of the cars have either very little mileage on the odometer or very much. Read it like this:\n\n- The *stem* is the vertical line.\n- The numbers to the left of the stem are, in this case, numbers in the sixth place to the left of the decimal point. In other words the first row represents zero to 999999.\n- Each character to the right of the stem represents one car. There are probably 80 zeros in the first row. The `+390151` indicates that there are 390,151 cars in that category that are not represented. The numbers in these cases represent the next significant digit after the one on the stem.\n- It is probably easier to read a stem and leaf plot for a smaller data frame, in the following case for the first 100 cars in the above data frame.\n\n```{r}\nstem(df$odometer[1:100])\n```\n\nThese entries come from a reduced data frame where I first ran the above code, getting rid of the high-priced and free cars. It may make it easier to understand to look at the entries themselves.\n\n```{r}\nhead(df$odometer,n=100L) |> sort(decreasing=TRUE)\n```\n\nThe very first row in the stem and leaf plot above counts cars priced at less than 20,000 dollars. There are 28 of them. They all have 0 or 1 in the fifth position to the left of the decimal. Only one of those, which is offered at 21 dollars, has zeros in both of the first two positions. It is the very first entry after the stem, represented as a zero. The next three entries are the cars that sell for the next lowest prices, between zero and 2 in the next decimal position. They are represented as 2s. You can see at a glance that, in this group of 100 cars, the lower odometer readings predominate. By the way, the `stem()` function discards `NA` values before processing the remainder. So there are only a total of 78 characters to the right of the stem on all the rows put together.\n\nThere is some difference of opinion as to whether `stem()` is graphical or numerical. What do you think?\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"include-in-header":{"text":"<style>\n#quarto-sidebar .menu-text {\n  display: flex;\n}\n#quarto-sidebar .chapter-number {\n  display: block;\n  width: 1.5rem;\n  text-align: right;\n}\n#quarto-sidebar .chapter-title {\n  display: block;\n  padding-left: 8px;\n  text-indent: -2px;\n  width: 100%;\n}\n</style>\n"},"output-file":"week03.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","bibliography":["master.bib"],"mainfont":"Tex Gyre Schola","monofont":"JetBrainsMono Nerd Font","mathfont":"Tex Gyre Schola Math","theme":"cosmo","title":"More about R and Quarto"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}