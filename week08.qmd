---
title: "Inference for Numerical Data"
---

```{r, include=FALSE}
knitr::opts_chunk$set(
  message=FALSE
)
library(tidyverse)
```

## Milestone 2 review

- There should be an overall summary at the end of the visual description section, rather than a separate overall summary from each group member (!?)
- Please make the .qmd file more readable by doing a few things like leaving a blank line before each header and leaving a blank line before and after each chunk.
- There should be no bar charts with two or three bars!
- There should be no pie charts with fewer than three slices nor more than seven slices.
- Please use headings (hashtags for levels 1, 2, and 3, for instance) rather than bold face text to mark sections. That way, I can view the structure of your document. Note that there are already level 1 and level 2 headings built in. You can add more.
- The visual description section should not have long lists of numbers but rather graphics.
- Any list of numbers more than two screensful is unreadable by a manager anyway.
- Group members need to spend more time and effort coordinating with each other.
- Explanations of your graphics help me believe that you understand what you are doing.
- The numerical description is supposed to be left in and improved where possible. You are trying to build the components of a single coherent report, being added to with each milestone.
- It is unwise to sign your individual names to the individual sections. This is supposed to be a group effort. You should be helping each other and reviewing the total package when it's assembled.
- There are techniques you can use to stay out of each other's way. For example, how could you avoid giving two data frames the same name? (Hint: include your initials in the names of data frames you generate.)
- The first two milestones are about description. It is not always reliable to make inferences from the descriptions. The second two milestones are about inferences. For example, it makes no sense to say that paint colors are evenly distributed among warm and cold states without doing a hypothesis test. A simple bar chart can provide a hint as to which hypothesis test you can run, but can't substitute for a hypothesis test.
- It is well known that it is hard to spot the differences between two versions of a file. That is why there is a whole industry of *diff* programs to highlight differences between files. You should each obtain and use some sort of *diff* program. For example, I use `vimdiff` because I use the Vim text editor. For relatively sophisticated text editors, there is likely to be a matching diff program or module. When you send me a file and I make changes to it, you should compare it to your original file using some version of diff. All information professionals should know about diff. For example, it is at the root of github, a website used by many information professionals.
- If you only do what you're explicitly told to do, you can and will be replaced by a bot. You have to be adventurous to be a valuable information professional.

## Individual Work 1 Review

### Product and Process
There's a difference between the product you produce and the process you go through to get there. Sometimes you need to include aspects of the process and sometimes you don't.

As an example of both, I investigated the bikes data. Through googling, I discovered that it actually came from Washington, DC. Since it began on January 1, 2011 and the day of the week was listed as 6, I opened a terminal on my Mac and said `cal jan 2011`, returning a calendar showing that January 1st was a Saturday. That way, I knew how to encode the days of the week as shown below, with Sunday as zero.

Next, I noticed that January 1st had a 1 in the `holiday` column, so I inferred that 1 means yes and 0 means no. So I coded the labels that way. The same was true of `workingday`.

The column `weathersit` was an interesting problem. Examining the lengthy description of that column, I decided to code it as an ordered factor, with one being the best weather and 4 being the worst.

Finally, I noticed that `temp` was coded in a way that made it hard for a Fahrenheit user like me to relate to. So I followed the formula in the description to convert the temperature to Fahrenheit. I did that in several lines so I could check my work and I left it in the code, so I could come back to it easily if I later discovered a problem with it.

Following is my creation of the types. Notice that I also saved a copy of the typed data in `bikes.RData` so I could quit R and quickly restart in case things got messed up.

```{r}
#| label: obtainingData
library(tidyverse)
df <- read_csv(paste0(Sys.getenv("STATS_DATA_DIR"),"/bikes.csv"))
df$instant <- as.integer(df$instant)
df$season <- factor(df$season,levels=1:4,labels=c("winter","spring","summer","fall"))
df$yr <- factor(df$yr,levels=0:1,labels=c("first","second"),ordered=TRUE)
df$mnth <- factor(df$mnth,levels=1:12,labels=c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"),ordered=TRUE)
df$hr <- factor(df$hr,levels=0:23,ordered=TRUE)
df$holiday <- factor(df$holiday,levels=0:1,labels=c("no","yes"))
df$weekday <- factor(df$weekday,levels=0:6,labels=c("Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"),ordered=TRUE)
df$workingday <- factor(df$workingday,levels=0:1,labels=c("no","yes"))
df$weathersit <- factor(df$weathersit,levels=1:4,labels=c("best","good","poor","worst"))
df$casual <- as.integer(df$casual)
df$registered <- as.integer(df$registered)
df$cnt <- as.integer(df$cnt)
t_min <- -8
t_max <- 39
#. df$temp == (t-t_min)/(t_max-t_min)
#. (t_max-t_min)*df$temp == t-t_min
df$celsius <- ((t_max-t_min)*df$temp)+t_min
df$fahr <- df$celsius*9/5+32
#. save(df,file="bikes.Rdata")
#. load(df,file="bikes.Rdata")
```

After setting up the types, I wanted to obtain a numerical summary of all the data. I started by saying things like `summary(df$temp)` but I noticed that it was pretty hard to read for a manager and the assignment spec calls for a report you can give to a manager. (Later, you'll learn how to make the reports even more manager-friendly.)

So I googled an expression like *r summary statistics for all columns* and immediately came across `vtable`, a package to do just that. So I read its documentation, which mentioned that there are many similar packages. Instead of searching for those packages and comparing them, I just tried the `st()` function in `vtable`, with the following result.

```{r}
library(vtable)
st(df)
```

I was fairly satisfied with that result, but it uncovered some errors in my data typing, so I went back and redid the previous step. Then I tried the `st()` function again, found more errors, and so on until I was completely satisfied that I had fixed everything.

Next, I made a couple of contingency tables. These made me think about the meaning of the rows in the data and the fact that not all hours are covered. I went back to the output of `st()` above and learned more before I made the following description for the contingency tables.

*The next two tables show the numbers of hours when riders experienced each weather condition, first during each season, then on each weekday.*

```{r}
addmargins(table(df$season,df$weathersit))
addmargins(table(df$weekday,df$weathersit))
```

After a few more boring contingency tables, I switched to visualizations. First, I just started aimlessly dong visualizations and looking at them. Let me emphasize that I wouldn't necessarily include these in my final report. They are just exploratory at this point to see if I find something interesting.

```{r}
df |> ggplot(aes(mnth,fahr)) + geom_line()
df |> ggplot(aes(mnth,cnt)) + geom_line()
df |> ggplot(aes(weathersit,cnt)) + geom_boxplot()
df |> ggplot(aes(season,cnt)) + geom_boxplot()
df |> ggplot(aes(weekday,cnt)) + geom_violin()
df |> ggplot(aes(hr,cnt)) + geom_violin()
df |> ggplot(aes(casual)) + geom_histogram()
df |> ggplot(aes(registered)) + geom_histogram()
```

Bingo! I have found something interesting in the last two plots above. But the way they're done doesn't reveal enough.
When drawing histograms of the casual riders and the registered riders, it really stands out that there's a hump in the registered riders, while there's exponential decay in the casual riders. You can illustrate this by combining the two, casual and registered, into a single density plot as follows.

```{r}
cas <- data.frame(vol=df$casual)
reg <- data.frame(vol=df$registered)
cas$type <- "casual"
reg$type <- "registered"
combined <- rbind(cas,reg)
ggplot(combined,aes(vol,fill=type)) + geom_density(alpha=0.3)
```

How did I come up with this? The main thing was a process of exploring the data through visualizing, making a discovery, then communicating that discovery. I would remove the two histograms (and most of the plots above them, too) and instead substitute the density plot in a report, never really showing the manager the process I went through.

Incidentally, I simple googled *tidyverse difference between two histograms* to come up with the code for the density plot, which I adapted from one of many good answers on [stackoverflow](https://stackoverflow.com/questions/3541713/how-to-plot-two-histograms-together-in-r).

### Common Mistakes
It's tough to grade this exercise because there's a tension between two factors. First is the amount of work you put in, which, in most cases, was a lot. I want to reward you for that.

Second is that you made a lot of common sense mistakes. There's a big difference between making mistakes in coding, which is only natural in a new language, and making mistakes that a manager unfamiliar with code will notice.

Let me list a few such mistakes here.

- Incredibly long contingency tables: Think about what a manager can possibly learn from the artifacts you produce---if a table goes on for many screensful of output, it doesn't do any more than presenting the raw data
- Graphics that are hard to read and look almost identical to each other: Think about what a manager can infer from a bunch of nearly identical graphics
- Explanations that don't make any sense considering the data, such as assuming that the left side of a graphic that doesn't include time refers to *early* data
- Explanations that are overly obvious, such as the fact that the data begins in winter and continues into summer

## Recap Week 07

- Inference for Categorical Data
  - Inference for a single proportion
  - Difference of two proportions
  - Testing goodness of fit using $\chi^2$
  - Testing independence of contingency tables

## Inference for numerical data
- Textbook section 7.1 One-sample means with the t-distribution
- Textbook section 7.2 Paired data
- Textbook section 7.3 Difference of two means
- Textbook section 7.4 Power calculations for a difference of means
- Textbook section 7.5 Comparing many means with ANOVA

### Textbook Section 7.1 One-sample means with the $t$-distribution
Modeling $\bar{x}$, the mean of a random sample from a normally distributed population, requires that the sample elements are

- independent---a random sample or a sample from a random process
- normally distributed---sample drawn from a normally distributed population

#### Rule of thumb for normality

- $n < 30$ and no outliers, assume data come from a normally distributed population
- $n \geqslant 30$ and no extreme outliers, assume $\bar{x}\sim N(\mu,\sigma)$ even if data come from a not normally distributed population

#### $t$-distribution

The $t$-distribution is useful for small samples ($n<30$). It was discovered when a man named Gossett was trying to figure out how few samples of beer he could get away with in tests for the Guinness brewery about 120 years ago. He preferred to remain anonymous at the time because he didn't want his employers to question his *outside* activities, otherwise this would probably be called the Gossett's $t$-distribution. Instead, he referred to himself as "A Student" so it came to be known as the Student's $t$-distribution.

For sample sizes over thirty, it converges to looking like the normal distribution, but for smaller samples, it gets more and more peaked and the tails get thicker and thicker. For example, here is the density function for a sample size of 4.

```{r}
plot(function(x) dt(x, df = 3), -11, 11, ylim = c(0, 0.50), main = "t density function", yaxs = "i")
```

Bear in mind that the `t()` function in R has nothing to do with the $t$-distribution (it's for transposing matrices and data frames). Instead, the functions for handling the $t$-distribution are the letter t prefaced by d, q, p, or r. You may have noticed that we saw functions like `pnorm()` and `dnorm()` when working with the normal distribution. These functions are analogous.

The $t$-distribution has $n-1$ degrees of freedom, so you can tell that the above example has $n=4$ since $\text{df}=3$.

Also keep in mind that the mean is always zero for the $t$-distribution, so it just has one parameter, df. So in the above example, you could say $\bar{x}\sim t(3)$.

Analogous to the `pnorm()` function, you can calculate regions of the $t$-distribution using the `pt()` function. For example, if you conduct a test that returns a $t$-statistic of $-2.10$ and $n=19$, you can use the following to find out that the area to the left of the statistic is 0.025. (This example and the two following are illustrated in textbook Figure 7.4.)

```{r}
pt(-2.1,18)
```

Suppose you obtain a $t$-statistic of 1.65 on 20 degrees of freedom. How much of the probability is in the upper tail? There are two obvious ways to do this.

```{r}
pt(1.65,20,lower.tail=FALSE)
1-pt(1.65,20)
```

Find the probability in both tails for a $t$-statistic of $\pm 3$ and two degrees of freedom.

```{r}
pt(-3,2)+pt(3,2,lower.tail=FALSE)
```

Textbook example 7.8 asks you to calculate the $t$-statistic when you know the proportion. In this case, $df=18$ and you want to know the $t$-statistic corresponding to 0.025 in the upper tail. You can use the `qt()` function where q stands for quantile. The region 0.025 in the upper tail corresponds to a 95 percent confidence interval because there will be 0.025 in each of the two tails for a total of five percent. The $t$-statistic for the lower tail would simply be the negative of the $t$-statistic for the upper tail.

```{r}
qt(0.025,18,lower.tail=FALSE)
```

To construct a confidence interval, you'll generally choose 90 or 95 percent, depending on the sensitivity of the real world problem. Then you'll plug that into the following formula.

$$
\bar{x} \pm t^*_{df}\frac{s}{\sqrt{n}}
$$

This assumes you have already checked the normality and independence constraints.

#### Calculating a confidence interval
For the textbook examples, you are given components of the formula. It is quite a bit simpler if you are given the raw data. For example, calculate a 95 percent confidence interval for the body mass in grams of the penguins in the Palmer penguins data frame.

```{r}
library(palmerpenguins)
model <- lm(body_mass_g ~ 1,penguins)
confint(model,level=0.95)
```

The above incantation may seem a little mysterious but you'll explore it in excruciating detail when you learn linear regression.

#### One sample $t$-tests
The textbook gives a lengthy example of the runner times of the Cherry Blossom race. I assume that the data are the `run10samp` and `run17samp` data frames from the textbook website, so I downloaded the RData versions of them and loaded them as follows.

```{r}
load(paste0(Sys.getenv("STATS_DATA_DIR"),"/run10samp.rda"))
load(paste0(Sys.getenv("STATS_DATA_DIR"),"/run17samp.rda"))
t.test(run17samp$clock_sec/60,mu=93.29,alternative="t")
```

Note that neither time measure, `clock_sec` nor `net_sec` correspond to the mean in the textbook. The $t$-statistic is smaller and the $p$-value is larger than that given in the textbook. With a $p$-value of 0.05075 it is unclear whether you would reject the null hypothesis. Certainly the old mean is within, though at the edge, of the confidence interval. Personally, I would fail to reject in this case.

### Textbook Section 7.2 Paired data
Suppose you want to know if two data frames were drawn from the same distribution or if they differ.

The textbook example is of the mean prices of textbooks on Amazon and in the UCLA campus bookstore. The data appear to be the textbooks data frame on the textbook website, although the statistics are different and the textbook says that there were two such samples (only one is on the website that I could find). Because they have precomputed the difference as the `diff` column, you can do this the same way as for a one sample test.

```{r}
load(paste0(Sys.getenv("STATS_DATA_DIR"),"/textbooks.rda"))
with(textbooks,t.test(diff))
```

Note that the default is that the difference is 0 and the 95 percent confidence interval is quite far from including 0. Also, the $p$-value is infinitesimal. We definitely reject the null hypothesis that the stores have similar prices.

If you didn't have the `diff` column, you could get the same result by saying the following.

```{r}
with(textbooks,t.test(ucla_new,amaz_new,paired=TRUE))
```

### Textbook Section 7.3 Difference of two means
In the previous section, you considered the means of the differences but in this section you consider the differences of the means. In the Amazon and UCLA example, the items were paired and we subtracted the price of a particular title sold by one seller from the price of the *same* title sold by the other seller. But what if we have data that is not paired like this? The textbook gives an example of a radical stem cell treatment given to sheep. One of two treatments is given to each sheep, but there is no correspondence between individual pairs of sheep.

In this case, there may be different variance between the two groups, as well as different means. So the standard error is calculated as

$$\text{SE}=\sqrt{\frac{\sigma^2_1}{n_1}+\frac{\sigma^2_2}{n_2}}$$

Notice that this formula implies that the samples could differ in size as well as variance.

For the sheep example, heart pumping capacity was measured, where more is better. The `stem_cell` data frame on the textbook website seems to be the appropriate data frame here. Conducting the test in R follows.

```{r}
load(paste0(Sys.getenv("STATS_DATA_DIR"),"/stem_cell.rda"))
with(stem_cell,t.test(after-before~trmt,var.equal=FALSE))
```

The result is a statistically significant difference between the sheep in the control group and the sheep treated with stem cells. The sheep in the stem cell group enjoyed a 3.5 unit increase in heart pumping capacity, while the poor sheep in the control group lost four and a third units. Of course, the practical question you have to ask yourself is whether these numbers have a practical significance. You would need domain knowledge to tell whether 3.5 units is a lot of heart pumping capacity!

### Textbook Section 7.4 Power calculations for a difference of means
The pictures in section 7.4, particularly the two on page 280 of the textbook, are essential for understanding power calculation, so let's use the textbook exclusively for this section. To do the calculations in R, you can use the `pwr` package.

### Textbook Section 7.5 Comparing many means with ANOVA
Textbook exercise 7.54 compares eight methods for loosening rusty bolts. Four samples were collected for each method and the results are in the `penetrating_oil` data frame on the textbook website. You can conduct an ANOVA test on the results using R as follows.

```{r}
load(paste0(Sys.getenv("STATS_DATA_DIR"),"/penetrating_oil.rda"))
with(penetrating_oil,anova(lm(torque~treatment)))
```

Here you are comparing whether any of the eight means of torque differ. They certainly seem to, with a large $F$-statistic and a small $p$-value. It might be helpful to visualize the differences with a combination violin plot and boxplot. There are really too few samples for each treatment.

```{r}
library(tidyverse)
penetrating_oil |>
  ggplot(aes(torque,treatment)) +
  geom_violin() +
  geom_boxplot(width=0.1)
```
